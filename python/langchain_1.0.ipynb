{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72ba22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e32e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (1.0.0a14)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0a7 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain) (1.0.0a8)\n",
      "Requirement already satisfied: langgraph<2.0.0,>=1.0.0a4 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain) (1.0.0a4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0a7->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0a7->langchain) (0.4.31)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0a7->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0a7->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0a7->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.0a7->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph<2.0.0,>=1.0.0a4->langchain) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt==0.7.0a2 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph<2.0.0,>=1.0.0a4->langchain) (0.7.0a2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph<2.0.0,>=1.0.0a4->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph<2.0.0,>=1.0.0a4->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0a7->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph<2.0.0,>=1.0.0a4->langchain) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0a7->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0a7->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0a7->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0a7->langchain) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0a7->langchain) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<2.0.0,>=1.0.0a4->langchain) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --pre -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b53b837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.0.0a13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56fa9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "from langsmith import traceable\n",
    "import base64\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43514a65",
   "metadata": {},
   "source": [
    "# create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"anthropic:claude-3-7-sonnet-latest\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c84523",
   "metadata": {},
   "source": [
    "## Additional fields in inputs renders weird \n",
    "- Doesnt have the \"Additional fields\" callout like the output does\n",
    "- Shows up BEFORE the list of messages (should be after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ad2d2",
   "metadata": {},
   "source": [
    "# init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4424fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_oai = init_chat_model(\"openai:gpt-4.1\")\n",
    "llm_oai.invoke(\"Why do parrots talk?\")\n",
    "\n",
    "model_oai = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1563a82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Parrots \"talk\" because they are highly intelligent and social birds capable of **vocal mimicry**—the ability to imitate sounds, including human speech. Here’s a more detailed explanation of why parrots talk:\\n\\n### 1. **Social Interaction**\\nIn the wild, parrots use a variety of sounds to communicate with their flock: warning calls, mating calls, signaling locations, forming bonds, etc. **When kept as pets, humans become their “flock,”** so parrots try to communicate with them using the sounds they hear most—often human speech.\\n\\n### 2. **Imitation and Learning**\\nParrots have an extraordinary ability to mimic sounds because of a unique organ in their throat called the **syrinx**. Their brains are also wired for learning and copying. When they hear frequently repeated words or phrases, especially those said with enthusiasm or emotion, they’re more likely to memorize and repeat them.\\n\\n### 3. **Attention and Reward**\\nParrots quickly learn that saying certain words or sounds gets a reaction—laughter, treats, attention, or excitement. This positive reinforcement encourages them to \"talk\" more.\\n\\n### 4. **Intelligence and Enrichment**\\nParrots are **exceptionally intelligent** animals; some species rival the cognitive abilities of young children. Talking and mimicking provide mental stimulation and enrichment, helping stave off boredom.\\n\\n---\\n\\n**In summary:**  \\nParrots talk as a natural extension of their social nature and intelligence. In captivity, they mimic human speech because it helps them bond with their human companions and because it’s fun and rewarding for them!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 324, 'prompt_tokens': 13, 'total_tokens': 337, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_e24a1fec47', 'id': 'chatcmpl-CQdfrm8jzJqVyEOemp36fCYmmlATZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c7ccc1b7-7529-4f34-b19b-2cc80e2a00b5-0', usage_metadata={'input_tokens': 13, 'output_tokens': 324, 'total_tokens': 337, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_oai_completions = init_chat_model(\"openai:gpt-4.1\", use_responses_api=False)\n",
    "llm_oai_completions.invoke(\"Why do parrots talk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fed986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Parrots don\\'t truly \"talk\" in the human sense - they mimic sounds they hear in their environment. This ability evolved as a social adaptation that helps them identify with their flock and establish territory in the wild.\\n\\nIn captivity, parrots often bond with humans as their flock, so they mimic human speech. Their vocal abilities are possible because they have a specialized vocal organ called the syrinx and well-developed brain regions for vocal learning. However, while parrots can associate some words with meanings (like \"treat\" with food), they don\\'t understand grammar or language the way humans do.', additional_kwargs={}, response_metadata={'id': 'msg_01P3gQjMyJMvcMGL3DXopqBj', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 14, 'output_tokens': 133, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic'}, id='lc_run--bed597f9-e43f-40e2-8656-8ce1e9bbeefa-0', usage_metadata={'input_tokens': 14, 'output_tokens': 133, 'total_tokens': 147, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_anthropic = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\n",
    "model_anthropic = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\n",
    "\n",
    "llm_anthropic.invoke(\"Why do parrots talk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f822054",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"id\": \"msg_123\",\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What breed is this dog?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image\",\n",
    "          \"source_type\": \"url\",\n",
    "          \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\",\n",
    "          \"mime_type\": \"image/jpeg\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "llm.invoke(inputs[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1fc87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1af4a70d",
   "metadata": {},
   "source": [
    "# model_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4a8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather at a location.\"\"\"\n",
    "    return f\"It's sunny in {location}.\"\n",
    "\n",
    "\n",
    "model_with_tools = llm_oai.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812cfde",
   "metadata": {},
   "source": [
    "## Output preview for oai is wrong. Shows kwargs.additional_kwargs.refusal\": null but should be tool_calls.args\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=8745caa4-9ae5-4bad-adda-1c4af9908577&peeked_trace=8745caa4-9ae5-4bad-adda-1c4af9908577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80642f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather at a location.\"\"\"\n",
    "    return f\"It's sunny in {location}.\"\n",
    "\n",
    "\n",
    "model_with_tools = llm_anth.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca677e3",
   "metadata": {},
   "source": [
    "## Output preview would be better as tool_calls.args\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=40bab8bc-a4a6-40ff-9c83-89a5444a60d0&peeked_trace=40bab8bc-a4a6-40ff-9c83-89a5444a60d0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742cdc2",
   "metadata": {},
   "source": [
    "# structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e30625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': AIMessage(content='{\"title\":\"Inception\",\"year\":2010,\"director\":\"Christopher Nolan\",\"rating\":8.8}', additional_kwargs={'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 126, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_e24a1fec47', 'id': 'chatcmpl-CQKIYaAlVDO7QuZzF8jhRNAShYV8a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--20152797-fdc1-4770-9047-7cddc7d7b137-0', usage_metadata={'input_tokens': 126, 'output_tokens': 22, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), 'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'parsing_error': None}\n"
     ]
    }
   ],
   "source": [
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = llm_oai.with_structured_output(Movie, include_raw=True)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb01065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': AIMessage(content=[{'id': 'toolu_01WFB1Qy828UkqSbntiQCU3W', 'input': {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}, 'name': 'Movie', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01NnqzUHGgL3inrzTkQEEEFL', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 465, 'output_tokens': 91, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic'}, id='lc_run--8e5f3133-8445-4a24-923c-602078823c67-0', tool_calls=[{'name': 'Movie', 'args': {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}, 'id': 'toolu_01WFB1Qy828UkqSbntiQCU3W', 'type': 'tool_call'}], usage_metadata={'input_tokens': 465, 'output_tokens': 91, 'total_tokens': 556, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}), 'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'parsing_error': None}\n"
     ]
    }
   ],
   "source": [
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = llm_anthropic.with_structured_output(Movie, include_raw=True)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a8faf",
   "metadata": {},
   "source": [
    "## Output preview currently shows empty AI message. Should show the array in \"parsed\"\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=94e62bba-30b4-401f-b50d-e402e42d22af&peeked_trace=94e62bba-30b4-401f-b50d-e402e42d22af\n",
    "\n",
    "\n",
    "(the output preview rendering worked for openai: https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=57ec9890-6a42-4001-b00d-bb90b07667db&peeked_trace=57ec9890-6a42-4001-b00d-bb90b07667db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6f7d8",
   "metadata": {},
   "source": [
    "# tools and tool messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca4cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a model makes a tool call\n",
    "ai_message = AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\": {\"location\": \"San Francisco\"},\n",
    "        \"id\": \"call_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Execute tool and create result message\n",
    "weather_result = \"Sunny, 72°F\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"  # Must match the call ID\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "messages = [\n",
    "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
    "    ai_message,  # Model's tool call\n",
    "    tool_message,  # Tool execution result\n",
    "]\n",
    "response = model_oai.invoke(messages)  # Model processes the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae81883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a model makes a tool call\n",
    "ai_message = AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\": {\"location\": \"San Francisco\"},\n",
    "        \"id\": \"call_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Execute tool and create result message\n",
    "weather_result = \"Sunny, 72°F\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"  # Must match the call ID\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "messages = [\n",
    "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
    "    ai_message,  # Model's tool call\n",
    "    tool_message,  # Tool execution result\n",
    "]\n",
    "response = model_anthropic.invoke(messages)  # Model processes the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd4071",
   "metadata": {},
   "source": [
    "# Content Blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46abbf45",
   "metadata": {},
   "source": [
    "## Reasoning and image inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "451d381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you for asking. \\n\\nI see you've shared an image of an adorable black Labrador puppy! The pup has a shiny black coat and those sweet, expressive eyes looking up at the camera. It appears to be resting on what looks like a wooden surface, possibly a deck or floor. Black Labs are known for being intelligent, friendly, and loyal companions. They're quite popular family dogs and are often great with children and other pets.\\n\\nIs this your puppy? They're absolutely precious!\", additional_kwargs={}, response_metadata={'id': 'msg_01H34fDod332Mih4413F5fxz', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 104, 'output_tokens': 119, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic'}, id='lc_run--1173ef4d-b69a-404b-8d57-8b90f6316979-0', usage_metadata={'input_tokens': 104, 'output_tokens': 119, 'total_tokens': 223, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# String content\n",
    "messages = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
    "    {\"type\": \"image\", \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\"},\n",
    "])] \n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e691565",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = {\n",
    "    \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": \"auto\",  # 'detailed', 'auto', or None\n",
    "}\n",
    "\n",
    "thinking={\"type\": \"enabled\", \"budget_tokens\": 2000}\n",
    "model_oai_content_blocks = init_chat_model(\"openai:gpt-5-nano\", output_version=\"v1\", reasoning=reasoning)\n",
    "#model_anthropic_content_blocks = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", output_version=\"v1\", thinking=thinking)\n",
    "model_anthropic_content_blocks = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", output_version=\"v1\")\n",
    "\n",
    "model_oai_completions_content_blocks = init_chat_model(\"openai:gpt-4.1\", use_responses_api=False, output_version=\"v1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4e483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'id': 'rs_0f50d63073ee0c9f0068eedfe4b98881a2962be71ac0b051fa', 'type': 'reasoning'}, {'type': 'text', 'text': 'A black dog (likely a puppy) lying on a wooden floor, looking up at the camera with big eyes.', 'annotations': [], 'id': 'msg_0f50d63073ee0c9f0068eedfe7579881a29b9673784fa9fcb1'}] additional_kwargs={} response_metadata={'id': 'resp_0f50d63073ee0c9f0068eedfe3991481a2a6753697258e3fd8', 'created_at': 1760485348.0, 'metadata': {}, 'model': 'gpt-5-nano-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'output_version': 'v1'} id='resp_0f50d63073ee0c9f0068eedfe3991481a2a6753697258e3fd8' usage_metadata={'input_tokens': 118, 'output_tokens': 349, 'total_tokens': 467, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "    {\"type\": \"image\", \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\"},\n",
    "])] \n",
    "\n",
    "response = model_oai_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "694d5cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'text', 'text': 'This image shows a black Labrador Retriever puppy lying on a wooden surface. The puppy is looking up toward the camera with a sweet and curious expression.'}] additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 268, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_e24a1fec47', 'id': 'chatcmpl-CQiemg2DbKIYvcuQFML8DDBmoJw8W', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None, 'output_version': 'v1'} id='lc_run--c8118215-ed51-4813-a731-b7cd62818e49-0' usage_metadata={'input_tokens': 268, 'output_tokens': 29, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = model_oai_completions_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0006fc",
   "metadata": {},
   "source": [
    "## Reasoning doesnt render (openai)\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=0d86fa2a-ac00-4741-bab9-31be10a726c7&peeked_trace=0d86fa2a-ac00-4741-bab9-31be10a726c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06caa358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'reasoning', 'reasoning': \"The image shows a black puppy, likely a Labrador Retriever or a similar breed, sitting or lying on what appears to be a wooden floor or surface. The puppy has a shiny black coat and dark eyes that give it an adorable, slightly sad expression. The puppy is looking directly at the camera. This is a close-up photo focusing on the puppy's face and upper body.\\n\\nI should describe what I see in the image, which is simply a black puppy dog on a wooden surface. There are no human faces in this image, so I can respond normally.\", 'extras': {'signature': 'ErUBCkYICBgCIkDvgR0FsxpIUwDdxBUrHWHCMnqedR1+Sx+HI166a6XwUeE7dZrgOygjG+RJXPepCA/uK+ZjakVJ//3UPyN24ZAtEgySuKtxhkaK3xMOLAAaDAJksA0b70fvhKpW6iIw6W4ZUI/7tATItObcgRI8SP/Jqf02AycHxXO+S+IVvoM2EvSOIIdfPegqEhHxmmKnKh3ihgt0kErAKbSugqjPLP6GWtFJrGwgm2t2ACdQlhgC'}}, {'type': 'text', 'text': \"This image shows an adorable black puppy lying on what appears to be a wooden surface. It has a shiny black coat and expressive dark eyes that give it a sweet, somewhat innocent look. The puppy appears to be a Labrador Retriever or similar breed, with characteristic floppy ears and a gentle face. It's looking directly at the camera with that classic puppy expression that many find irresistible.\"}] additional_kwargs={} response_metadata={'id': 'msg_01EuBtykAL3pA3X72WgocSrX', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 133, 'output_tokens': 227, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic', 'output_version': 'v1'} id='lc_run--b9f326ef-f3b4-42bc-97bf-ed30b0b5b271-0' usage_metadata={'input_tokens': 133, 'output_tokens': 227, 'total_tokens': 360, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "    {\"type\": \"image\", \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\"},\n",
    "])] \n",
    "\n",
    "response = model_anthropic_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c7e3e",
   "metadata": {},
   "source": [
    "## Reasoning doesnt render (anthropic)\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3095fff",
   "metadata": {},
   "source": [
    "## PDF inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e02d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'id': 'rs_0dc5ca941f66819f0068eedf8f2c7c81918381ffd97aa3c318', 'type': 'reasoning', 'reasoning': '**Interpreting image content**\\n\\nI see the user asked, \"What is in this image?\" They provided some sample text that looks like it\\'s from a PDF. The image capabilities are enabled, so maybe there\\'s an image attached, but I only see text. I should respond by describing it as a sample PDF with Lorem ipsum text. The user might expect a brief description like, \"It\\'s a sample PDF document containing placeholder text with repeated Latin phrases.\" I\\'ll keep it concise and clear!'}, {'id': 'rs_0dc5ca941f66819f0068eedf8f2c7c81918381ffd97aa3c318', 'type': 'reasoning', 'reasoning': \"**Describing the PDF sample**\\n\\nI need to answer that it's a simple PDF sample file with Lorem ipsum text, showing long paragraphs of placeholder text. There's no meaningful content here; it’s just standard filler. I could also ask the user if they want me to extract topics, but since it’s all filler, there’s not much to extract. I should keep my response clear: it's a sample PDF containing multiple paragraphs of Lorem ipsum, typical in publishing.\"}, {'id': 'rs_0dc5ca941f66819f0068eedf8f2c7c81918381ffd97aa3c318', 'type': 'reasoning', 'reasoning': '**Summarizing the PDF document**\\n\\nI can provide a concise answer along with a shorter bullet list for clarity:\\n\\n- Document type: PDF\\n- Content: Lorem ipsum filler text; no real information\\n- Length: long multi-paragraph blocks\\n\\nThe final response might be: \"This appears to be a simple sample PDF containing long blocks of Lorem ipsum filler text, which is just placeholder content. It\\'s not an image with graphics, just text. The user seems to be seeking a description of what\\'s in this PDF, so I\\'ll keep it straightforward.\" Overall, it\\'s all placeholder text.'}, {'type': 'text', 'text': 'This appears to be a simple sample PDF containing long blocks of Lorem ipsum placeholder text. There are no real details or images—just filler Latin text used to illustrate layout.', 'annotations': [], 'id': 'msg_0dc5ca941f66819f0068eedf992c8881918736f98cde882eac'}] additional_kwargs={} response_metadata={'id': 'resp_0dc5ca941f66819f0068eedf8e57048191b90885927691836b', 'created_at': 1760485262.0, 'metadata': {}, 'model': 'gpt-5-nano-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'output_version': 'v1'} id='resp_0dc5ca941f66819f0068eedf8e57048191b90885927691836b' usage_metadata={'input_tokens': 753, 'output_tokens': 488, 'total_tokens': 1241, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 448}}\n"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "    {\"type\": \"file\", \"url\": \"https://pdfobject.com/pdf/sample.pdf\"},\n",
    "])] \n",
    "\n",
    "response = model_oai_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cc69f",
   "metadata": {},
   "source": [
    "### PDF doesnt render \n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&runtab=0&peek=a4e20cab-9cf9-41a3-ae5d-ecf4c7a97f21&peeked_trace=a4e20cab-9cf9-41a3-ae5d-ecf4c7a97f21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8faa2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"What is in this pdf?\"},\n",
    "    {\"type\": \"file\", \"url\": \"https://pdfobject.com/pdf/sample.pdf\"},\n",
    "])] \n",
    "\n",
    "response = model_anthropic_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d46da7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanushreesharma/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_core/messages/block_translators/openai.py:82: UserWarning: OpenAI may require a filename for file uploads. Specify a filename in the content block, e.g.: {'type': 'file', 'mime_type': '...', 'base64': '...', 'filename': 'my-file.pdf'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Missing required parameter: 'messages[0].content[1].file.file_id'.\", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].file.file_id', 'code': 'missing_required_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m pdf_data = base64.b64encode(httpx.get(pdf_url).content).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m inputs = [HumanMessage(content_blocks=[\n\u001b[32m      5\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is in this pdf?\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbase64\u001b[39m\u001b[33m\"\u001b[39m: pdf_data, \u001b[33m\"\u001b[39m\u001b[33mmime_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/pdf\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      7\u001b[39m ])] \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mmodel_oai_completions_content_blocks\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:406\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m     **kwargs: Any,\n\u001b[32m    400\u001b[39m ) -> AIMessage:\n\u001b[32m    401\u001b[39m     config = ensure_config(config)\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    403\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    404\u001b[39m         cast(\n\u001b[32m    405\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    416\u001b[39m         ).message,\n\u001b[32m    417\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1113\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1110\u001b[39m     **kwargs: Any,\n\u001b[32m   1111\u001b[39m ) -> LLMResult:\n\u001b[32m   1112\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:928\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    927\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m         )\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    936\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1217\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1215\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1221\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1228\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1227\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1228\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1231\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1233\u001b[39m ):\n\u001b[32m   1234\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1223\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1217\u001b[39m             response,\n\u001b[32m   1218\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1219\u001b[39m             metadata=generation_info,\n\u001b[32m   1220\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1221\u001b[39m         )\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1223\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m         response = raw_response.parse()\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ls-tracing-testing/.venv/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Missing required parameter: 'messages[0].content[1].file.file_id'.\", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].file.file_id', 'code': 'missing_required_parameter'}}"
     ]
    }
   ],
   "source": [
    "pdf_url = \"https://pdfobject.com/pdf/sample.pdf\"\n",
    "pdf_data = base64.b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"What is in this pdf?\"},\n",
    "    {\"type\": \"file\", \"base64\": pdf_data, \"mime_type\": \"application/pdf\"},\n",
    "])] \n",
    "\n",
    "response = model_oai_completions_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e0f8dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'text', 'text': 'This is a PDF document titled \"Sample PDF\" with the subtitle \"This is a simple PDF file. Fun fun fun.\" The document contains several paragraphs of Lorem ipsum placeholder text, which is commonly used as filler text in publishing and graphic design. The text is formatted in a standard document layout with paragraphs of Latin-like text that doesn\\'t convey any specific meaning. It appears to be a demonstration or sample PDF file that might be used for testing purposes.'}] additional_kwargs={} response_metadata={'id': 'msg_01XwDkkKaJf6i5SBXZ2M2bVt', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2679, 'output_tokens': 100, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic', 'output_version': 'v1'} id='lc_run--db852d70-589d-426e-b5fe-442d945d7f5a-0' usage_metadata={'input_tokens': 2679, 'output_tokens': 100, 'total_tokens': 2779, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "    {\"type\": \"file\", \"url\": \"https://pdfobject.com/pdf/sample.pdf\"},\n",
    "])] \n",
    "\n",
    "response = model_anthropic_content_blocks.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66dad5",
   "metadata": {},
   "source": [
    "### PDF doesnt render \n",
    "\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&runtab=0&peek=db852d70-589d-426e-b5fe-442d945d7f5a&peeked_trace=db852d70-589d-426e-b5fe-442d945d7f5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9214ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'text', 'text': \"That’s a great reference! It sounds like you're channeling a famous quote from a popular animated show. The laws of thermodynamics, of course, are the pillars of understanding energy and heat in any system. \\n\\n1. Energy cannot be created or destroyed – only transformed.\\n2. In isolated systems, entropy (disorder) tends to increase.\\n3. As temperature approaches absolute zero, the entropy of a system approaches a constant minimum.\\n\\nTogether, they help us make sense of everything from engines to refrigerators to stars.\\n\\nSo rest assured—thermodynamics rules the house!\"}] additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 44, 'total_tokens': 159, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'text_tokens': 115}, 'prompt_tokens_details': {'audio_tokens': 33, 'cached_tokens': 0, 'text_tokens': 11, 'image_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-audio-preview-2025-06-03', 'system_fingerprint': 'fp_363417d4a6', 'id': 'chatcmpl-CQjS0VCuldJHy9YZ4aQmmOJsLLIFP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None, 'output_version': 'v1'} id='lc_run--3a2cf2c3-e96d-4658-921d-738cb313ed20-0' usage_metadata={'input_tokens': 44, 'output_tokens': 115, 'total_tokens': 159, 'input_token_details': {'audio': 33, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_oai_content_blocks_audio = init_chat_model(\"openai:gpt-4o-audio-preview\", output_version=\"v1\")\n",
    "\n",
    "\n",
    "pdf_url = \"https://people.math.sc.edu/Burkardt/data/wav/thermo.wav\"\n",
    "pdf_data = base64.b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "inputs = [HumanMessage(content_blocks=[\n",
    "    {\"type\": \"audio\", \"base64\": pdf_data, \"mime_type\": \"audio/wav\"},\n",
    "])] \n",
    "\n",
    "response = model_oai_content_blocks_audio.invoke(inputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb35ae",
   "metadata": {},
   "source": [
    "## we don't show the audio file in pretty rendering\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&runtab=0&peek=3a2cf2c3-e96d-4658-921d-738cb313ed20&peeked_trace=3a2cf2c3-e96d-4658-921d-738cb313ed20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377d103",
   "metadata": {},
   "source": [
    "## streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bf7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Par|rots| are| colorful| mainly| because| of| how| their| feathers| get| color| and| what| those| colors| are| used| for|.\n",
      "\n",
      "|How| the| colors| come| about|\n",
      "|-| Pig|ments|:\n",
      "| | -| Car|ot|eno|ids| (|from| diet|)| give| reds|,| oranges|,| yell|ows|.\n",
      "| | -| Mel|an|ins| produce| blacks| and| brow|ns|.\n",
      "| | -| Ps|itt|ac|of|ul|v|ins| (|produ|ced| by| parro|ts| themselves|)| can| give| reds|,| pink|s|,| and| yell|ows| in| some| species|.\n",
      "|-| Structural| coloration|:\n",
      "| | -| Tiny| feather| structures| break| up| and| reflect| light| to| produce| blues| and| ir|ides|cent| greens|.\n",
      "|-| Sometimes| UV| reflection|:\n",
      "| | -| Birds| can| see| ultraviolet| light|,| so| some| plum|age| patterns| look| very| different| to| them| than| to| us|.\n",
      "\n",
      "|Why| the| colors| matter|\n",
      "|-| Cam|ouflage|:| Green| bodies| help| parro|ts| blend| in| with| leaves| and| branches| in| forests|.\n",
      "|-| Communication| and| social| signaling|:| Bright| patches| help| individuals| recognize| each| other|,| attract| mates|,| and| signal| health| or| dominance|.\n",
      "|-| Species| and| mate| recognition|:| Dist|inct| color| patterns| help| parro|ts| identify| members| of| their| own| species| and| choose| mates|.\n",
      "|-| Health| and| diet| indicator|:| Bright| colors| (|especially| carot|enoid|-based|)| can| indicate| good| nutrition| and| overall| health|.\n",
      "\n",
      "|In| short|,| par|rot| colors| come| from| pigments| and| feather| structure|,| and| they| help| with| hiding|,| attracting| mates|,| signaling| health|,| and| keeping| social| groups| organized|.|||"
     ]
    }
   ],
   "source": [
    "for chunk in model_oai_content_blocks.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a27ab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Par|rots| have| colorful| feathers| for| several| reasons| related| to| **|e|volution|,| communication|,| and| survival|**|:\n",
      "\n",
      "|1|.| **|M|ating| and| Attraction|**|:| Bright|ly| colored| feathers| help| parro|ts| attract| mates|.| Many| par|rot| species| rely| on| visual| signals| for| court|ship|,| so| vibrant| colors| indicate| health|,| genetic| fitness|,| and| suitability| as| a| breeding| partner|.\n",
      "\n",
      "|2|.| **|Species| and| Individual| Recognition|**|:| The| variety| of| colors| and| patterns| helps| parro|ts| recognize| members| of| their| own| species| and| sometimes| even| distinguish| individuals| within| a| group|.\n",
      "\n",
      "|3|.| **|Cam|ouflage|**|:| Surprisingly|,| vivid| feather| colors| can| also| help| parro|ts| blend| into| their| natural| habitats|,| such| as| the| bright| greens|,| reds|,| and| yell|ows| found| in| rainforest| can|opies| and| flowers|.| These| colors| may| break| up| the| outline| of| the| bird|,| making| it| harder| for| predators| to| spot| them| among| the| foliage|.\n",
      "\n",
      "|4|.| **|Social| Sign|aling|**|:| Fe|athers| can| communicate| social| status|,| emotional| state|,| or| readiness| to| breed| to| other| parro|ts|.\n",
      "\n",
      "|5|.| **|Warning| or| Det|err|ence|**|:| Some| parro|ts| use| bright| colors| as| a| warning| to| potential| predators|,| signaling| that| they| might| be| difficult| to| catch|,| aggressive|,| or| potentially| un|pal|atable|.\n",
      "\n",
      "|**|How| the| colors| are| produced|:**|  \n",
      "|Par|rot| feather| colors| are| created| both| by| pigments| and| by| microscopic| structures| that| reflect| and| scatter| light| (|struct|ural| coloration|).| Special| pigments| called| **|ps|itt|ac|of|ul|v|ins|**| are| unique| to| parro|ts| and| give| them| their| vibrant| reds|,| oranges|,| and| yell|ows|.\n",
      "\n",
      "|In| summary|:| *|Color|ful| feathers| help| parro|ts| survive| and| thrive| by| providing| advantages| in| attraction|,| recognition|,| communication|,| and| camouflage| in| their| environments|.*||||"
     ]
    }
   ],
   "source": [
    "for chunk in model_oai_completions_content_blocks.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a415f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||# Why Parrots Have| Colorful Feathers|\n",
      "\n",
      "Parrots|'| vib|rant pl|umage serves| several important biological purposes|:\n",
      "\n",
      "## Sexual| Selection|\n",
      "The| bright colors often| play| a crucial| role in attract|ing mates. More| vib|rant individuals| may signal| better| health an|d genetic quality| to potential partners.|\n",
      "\n",
      "## Species Recognition|\n",
      "Distinctive color patterns help| parrots identify members| of their own species in| dense| forests,| which| is essential| for social interaction| and finding compatible| mates.\n",
      "\n",
      "##| Camouflage|\n",
      "Though| it| might seem counterintu|itive, bright colors can| provide| effective camouflage| in tropical| forests| full| of colorful fruits an|d flowers.|\n",
      "\n",
      "## Social| Communication|\n",
      "Within| parrot communities|, color| variations| can| signal age|, status, an|d health to other| members| of| their| f|lock.|\n",
      "\n",
      "Inter|estingly, parr|ots produce| unique| pigments called psitt|acofulvins| that create their red,| orange, and yellow col|oration -| these pigments are| foun|d only| in parrots an|d may offer| additional| benefits like| resistance| to feather-degra|ding bacteria.||"
     ]
    }
   ],
   "source": [
    "for chunk in model_anthropic_content_blocks.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbf076",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model_oai).stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e41eb5",
   "metadata": {},
   "source": [
    "##  structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da92fb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': AIMessage(content=[{'id': 'rs_0e2b00121d7abfd30068eedd2ba14481a2a2bb1a7f0cc9b5d0', 'type': 'reasoning', 'reasoning': '**Structuring movie details**\\n\\nThe user wants details about the movie Inception. I should respond in the structured \"Movie\" JSON format, which includes fields like title, year, director, and rating. Since rating could be around 8.8, I want to confirm the format before finalizing it. The instructions mention the JSON schema, so a structured response would likely be expected. I’ll make sure it aligns with the schema while keeping it clear and easy to understand!'}, {'id': 'rs_0e2b00121d7abfd30068eedd2ba14481a2a2bb1a7f0cc9b5d0', 'type': 'reasoning', 'reasoning': '**Defining JSON output format**\\n\\nThe instruction clarifies that the output must be parsed by code or shown in an app, which may not support special formatting like bullets. I need to follow the \"Movie\" JSON schema precisely, including keys such as title, year, director, and rating. For instance, I’d represent \"Inception\" as \"title\": \"Inception\", \"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.8. Although there\\'s a request for details, I should stick to the specified fields in the schema.'}, {'id': 'rs_0e2b00121d7abfd30068eedd2ba14481a2a2bb1a7f0cc9b5d0', 'type': 'reasoning', 'reasoning': \"**Choosing movie details**\\n\\nI need to fill in the four required fields for the JSON output. Let’s go with a rating of 8.8 since that’s the widely accepted IMDb rating for Inception as of 2024. The year is 2010, and the director is Christopher Nolan. The user's request for details could imply they want a bit more than just the basics. Although the schema is minimal, providing a brief summary alongside the structured JSON might be helpful, since the user didn’t specify a format. However, the app might expect strict JSON conformity, so I'll consider that too.\"}, {'id': 'rs_0e2b00121d7abfd30068eedd2ba14481a2a2bb1a7f0cc9b5d0', 'type': 'reasoning', 'reasoning': '**Ensuring proper format**\\n\\nI want to make sure I avoid conflicting formats. If I include both natural language and JSON, the app might misparse the output. The instructions suggest that the response needs to be structured data, particularly as they\\'re meant for an app that may not handle special formatting. While the user asked for \"details,\" we’ll stick to the required JSON fields. I’ll create the object with the title, year, director, and rating, so it looks like this:\\n\\n{\"title\":\"Inception\",\"year\":2010,\"director\":\"Christopher Nolan\",\"rating\":8.8}.'}, {'id': 'rs_0e2b00121d7abfd30068eedd2ba14481a2a2bb1a7f0cc9b5d0', 'type': 'reasoning', 'reasoning': '**Finalizing JSON format**\\n\\nI need to make sure the JSON output adheres to the correct case for the keys: \"title,\" \"year,\" \"director,\" and \"rating,\" with their appropriate values. So, I\\'ll finalize it as:\\n\\n{\"title\":\"Inception\",\"year\":2010,\"director\":\"Christopher Nolan\",\"rating\":8.8}.\\n\\nI’ve confirmed that the rating type is indeed a number. It might be helpful to mention that Inception is a 2010 science fiction heist thriller, but the schema limits us to only those four fields. I’ll stick to just providing the JSON object and keep it minimal, as instructed.'}, {'type': 'text', 'text': '{\"title\":\"Inception\",\"year\":2010,\"director\":\"Christopher Nolan\",\"rating\":8.8}', 'annotations': [], 'id': 'msg_0e2b00121d7abfd30068eedd3c3c0881a2bd4a32da68383f05'}], additional_kwargs={'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8)}, response_metadata={'id': 'resp_0e2b00121d7abfd30068eedd2b388c81a2b152d7ab7f98a989', 'created_at': 1760484651.0, 'metadata': {}, 'model': 'gpt-5-nano-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'output_version': 'v1'}, id='resp_0e2b00121d7abfd30068eedd2b388c81a2b152d7ab7f98a989', usage_metadata={'input_tokens': 118, 'output_tokens': 927, 'total_tokens': 1045, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 896}}), 'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'parsing_error': None}\n"
     ]
    }
   ],
   "source": [
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model_oai_content_blocks.with_structured_output(Movie, include_raw=True)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be8c8e",
   "metadata": {},
   "source": [
    "## we show too much in the pretty rendering \n",
    "\n",
    "Would remove all fields under \"response_metadata\", \"tool_calls\", \"usage_metadata\"\n",
    "\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&runtab=0&peek=70840561-16d0-4de5-a6b1-21e39d5de2d9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ba2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': AIMessage(content=[{'type': 'text', 'text': '{\"title\":\"Inception\",\"year\":2010,\"director\":\"Christopher Nolan\",\"rating\":8.8}'}], additional_kwargs={'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 126, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_e24a1fec47', 'id': 'chatcmpl-CQimmeL9epyYNpLXtlBv219q7vkdC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None, 'output_version': 'v1'}, id='lc_run--9ace9590-b011-44a8-85d2-b01ee024d813-0', usage_metadata={'input_tokens': 126, 'output_tokens': 22, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), 'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'parsing_error': None}\n"
     ]
    }
   ],
   "source": [
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model_oai_completions_content_blocks.with_structured_output(Movie, include_raw=True)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406c31d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': AIMessage(content=[{'type': 'tool_call', 'name': 'Movie', 'args': {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}, 'id': 'toolu_016EAVVcgrwEpaEu3uND8bUT'}], additional_kwargs={}, response_metadata={'id': 'msg_01WAHxEwpLhye5gCBrXCASR5', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 465, 'output_tokens': 91, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic', 'output_version': 'v1'}, id='lc_run--f7899a2f-8f5f-4118-b705-b122dbb608e9-0', tool_calls=[{'name': 'Movie', 'args': {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}, 'id': 'toolu_016EAVVcgrwEpaEu3uND8bUT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 465, 'output_tokens': 91, 'total_tokens': 556, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}), 'parsed': Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8), 'parsing_error': None}\n"
     ]
    }
   ],
   "source": [
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model_anthropic_content_blocks.with_structured_output(Movie, include_raw=True)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee793011",
   "metadata": {},
   "source": [
    "## we show too much in the pretty rendering \n",
    "\n",
    "Would remove all fields under \"response_metadata\", \"tool_calls\", \"usage_metadata\"\n",
    "\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&runtab=0&peek=7183b1d4-da3f-4baa-a614-1c3d9050b2fe&peeked_trace=7183b1d4-da3f-4baa-a614-1c3d9050b2fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10e7f1",
   "metadata": {},
   "source": [
    "## server-side tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'server_tool_call',\n",
       "  'name': 'web_search',\n",
       "  'args': {'query': 'positive news story today', 'type': 'search'},\n",
       "  'id': 'ws_0465e447a69b454e0068ed78255b60819f9508ba0ef0d9f863'},\n",
       " {'type': 'server_tool_result',\n",
       "  'tool_call_id': 'ws_0465e447a69b454e0068ed78255b60819f9508ba0ef0d9f863',\n",
       "  'status': 'success'},\n",
       " {'type': 'text',\n",
       "  'text': 'A positive news story from today is the release of all 20 living Israeli hostages held by Hamas in Gaza. This development marks a significant step toward peace, as Israel has agreed to release 250 Palestinian prisoners and over 1,700 detainees in exchange. ([en.wikipedia.org](https://en.wikipedia.org/wiki/2025?utm_source=openai))\\n\\nAdditionally, in Flowery Branch, Georgia, a high school community came together to support a security guard in financial need by providing her with a new car. This act of kindness was initiated by students and supported by local businesses. ([soundcloud.com](https://soundcloud.com/user-423075597/uplifting-and-771571133?utm_source=openai))\\n\\nFurthermore, rapper Pharrell Williams partnered with Adidas and other organizations to offer free swim and surf lessons to children and distribute free merchandise in Virginia Beach. ([soundcloud.com](https://soundcloud.com/user-423075597/uplifting-and-771571133?utm_source=openai))\\n\\nThese stories highlight the positive actions and community support taking place today. ',\n",
       "  'annotations': [{'end_index': 331,\n",
       "    'start_index': 257,\n",
       "    'title': '2025',\n",
       "    'type': 'citation',\n",
       "    'url': 'https://en.wikipedia.org/wiki/2025?utm_source=openai'},\n",
       "   {'end_index': 673,\n",
       "    'start_index': 574,\n",
       "    'title': 'Stream Uplifting and Positive News October 13th, 2025 by The Positive Truth| Uplifting News| Positive News | Listen online for free on SoundCloud',\n",
       "    'type': 'citation',\n",
       "    'url': 'https://soundcloud.com/user-423075597/uplifting-and-771571133?utm_source=openai'},\n",
       "   {'end_index': 957,\n",
       "    'start_index': 858,\n",
       "    'title': 'Stream Uplifting and Positive News October 13th, 2025 by The Positive Truth| Uplifting News| Positive News | Listen online for free on SoundCloud',\n",
       "    'type': 'citation',\n",
       "    'url': 'https://soundcloud.com/user-423075597/uplifting-and-771571133?utm_source=openai'}],\n",
       "  'id': 'msg_0465e447a69b454e0068ed78265674819f80b2f0764dc459ef'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openai: web search\n",
    "model_oai_content_blocks_tools = init_chat_model(\"openai:gpt-4.1-mini\", output_version=\"v1\")\n",
    "\n",
    "tool = {\"type\": \"web_search\"}\n",
    "model_with_tools = model_oai_content_blocks_tools.bind_tools([tool])\n",
    "\n",
    "response = model_with_tools.invoke(\"What was a positive news story from today?\")\n",
    "response.content_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b7485",
   "metadata": {},
   "source": [
    "## we don't render hyperlinks for citations correctly \n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=c35f72c4-d8e8-4463-9482-6f82a13603dd&peeked_trace=c35f72c4-d8e8-4463-9482-6f82a13603dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94224ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai: image generation\n",
    "tool = {\"type\": \"image_generation\", \"quality\": \"low\"}\n",
    "\n",
    "llm_with_tools = model_oai_content_blocks_tools.bind_tools([tool])\n",
    "\n",
    "ai_message = llm_with_tools.invoke(\n",
    "    \"Draw a picture of a cute fuzzy cat with an umbrella\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77350edb",
   "metadata": {},
   "source": [
    "## we don't render the base64 image output\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=074e9352-afc3-4b59-bccb-b6967bec9184&peeked_trace=074e9352-afc3-4b59-bccb-b6967bec9184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai:code interpreter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_with_tools = model_oai_content_blocks_tools.bind_tools(\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"code_interpreter\",\n",
    "            # Create a new container\n",
    "            \"container\": {\"type\": \"auto\"},\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "response = llm_with_tools.invoke(\n",
    "    \"Write and run code to answer the question: what is 3^3?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5448341",
   "metadata": {},
   "source": [
    "## Tools appears empty\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=17fd9dc7-4630-4112-a378-f28884308cee&peeked_trace=17fd9dc7-4630-4112-a378-f28884308cee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56ab3b",
   "metadata": {},
   "source": [
    "### didnt test openai file search, computer use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "648c46df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'reasoning', 'reasoning': 'This document is an announcement or flyer for a workshop event titled \"Hands-on Workshop: Evaluating AI in High-Stakes Applications.\" Let me analyze its key components:\\n\\n1. Event Details:\\n   - Title: \"Hands-on Workshop: Evaluating AI in High-Stakes Applications\"\\n   - Date: September 16, 2025\\n   - Time: 8:45-10:15 AM\\n   - Location: QED Building, SF\\n   - Note: Breakfast provided\\n   - Facilitation: BikeShop@MIT as part of the EDGES Summit\\n\\n2. Purpose Section (\"What This Is\"):\\n   - An invite-only, off-the-record exchange between AI practitioners working with user-facing systems in high-stakes contexts\\n   - Focus on settings where AI assists humans in making important decisions\\n   - Examples mentioned: complex customer service, financial advisory, legal support, health, career coaching, sales enablement\\n   - No pitches or demos—just honest conversations about what works and what doesn\\'t\\n   - Bringing together four perspectives: startup teams with customer learnings, social entrepreneurs working with vulnerable populations, AI researchers, and evaluation tool vendors\\n\\n3. \"Why Join\" Section:\\n   - Learn from peers facing similar AI evaluation challenges\\n   - Learn about research frontiers and provide input for MIT\\'s new lab on human-AI collaboration\\n   - Get practical insights on evaluation approaches that correlate with user outcomes\\n\\n4. \"What We\\'ll Dig Into\" Section:\\n   - Multi-Turn Evaluation in the Wild: Testing whether AI maintains context and intent across real conversations\\n   - Beyond Vanity Metrics: Discussing outcome-based metrics that matter in high-stakes domains\\n   - Human vs AI Feedback at Scale: When to use human rubrics vs automated reward models\\n\\n5. \"About the Hosts\" Section:\\n   - Responsible Innovation Lab: Builds \"category-defining, society-positive startups\"; founded by alumni from Stripe, Coinbase, and General Catalyst\\n   - BikeShop@MIT: New AI research lab funded by Gates Foundation focused on AI that expands rather than replaces human potential\\n\\n6. Footer note:\\n   - The workshop is part of EDGES Summit, described as \"an intimate gathering for leaders at the frontiers of technology and human achievement\"\\n\\nThe document has a professional design with a blue/purple gradient background and a dark gray/white content area. The layout is clean with clear section headings and concise descriptions.', 'extras': {'signature': 'ErUBCkYICBgCIkB+i6Fa5gwMokpR26GfJkBHz/cdSwlqQs9TPCJbuDjvQBmjnxy9+E8Qm2qlalGx37L3JTyTGLDtCsuHaGFIKHPcEgwcsSQRJ8IehwWNwWIaDNDRyM19QyF5/hGz/iIw0UGck1ydvSmx/lmQ4deaKGYn2y2drTB866M+ca0wt7qBwAxAhKOVKYpr3MxjJWV1Kh3AS856K7U37zG0DTsAulNEaPJl+kmm7SoXJJgRwBgC'}}, {'type': 'text', 'text': '# Workshop Flyer: Evaluating AI in High-Stakes Applications\\n\\nThis document is a promotional flyer for a hands-on workshop focused on evaluating AI in high-stakes applications. Here are the key details:\\n\\n## Event Information\\n- **Title:** Hands-on Workshop: Evaluating AI in High-Stakes Applications\\n- **Date & Time:** September 16, 2025, 8:45-10:15 AM\\n- **Location:** QED Building, San Francisco\\n- **Note:** Breakfast provided\\n- **Facilitator:** BikeShop@MIT (as part of the EDGES Summit)\\n\\n## Workshop Description\\nThe event is described as an invite-only, off-the-record exchange between AI practitioners who are implementing user-facing systems in high-stakes contexts. It focuses on areas where AI helps humans make important decisions, such as customer service, financial advisory, legal support, healthcare, and career coaching.\\n\\n## Participant Benefits\\nThe flyer outlines several reasons to join:\\n- Learning from peers facing similar AI evaluation challenges\\n- Contributing to MIT\\'s research on human-AI collaboration\\n- Gaining tactical insights on effective evaluation approaches\\n\\n## Workshop Topics\\nThe workshop will cover three main areas:\\n1. **Multi-Turn Evaluation in the Wild** - Testing AI context maintenance in conversations\\n2. **Beyond Vanity Metrics** - Exploring outcome-based metrics in high-stakes domains\\n3. **Human vs AI Feedback at Scale** - Comparing human rubrics and automated reward models\\n\\n## Host Organizations\\n1. **Responsible Innovation Lab** - Building \"society-positive startups,\" founded by alumni from Stripe, Coinbase, and General Catalyst\\n2. **BikeShop@MIT** - An AI research lab funded by Gates Foundation, focused on AI that enhances rather than replaces human potential\\n\\nThe document has a professional design with a purple/blue gradient background and is formatted as part of the EDGES Summit, described as \"an intimate gathering for leaders at the frontiers of technology and human achievement.\"'}], additional_kwargs={}, response_metadata={'id': 'msg_011kEtbvGtn4o3sM3F3Cmc8s', 'container': None, 'context_management': None, 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1613, 'output_tokens': 993, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219', 'model_provider': 'anthropic', 'output_version': 'v1'}, id='lc_run--22d55efa-c44f-4459-b398-7fb30df66dfc-0', usage_metadata={'input_tokens': 1613, 'output_tokens': 993, 'total_tokens': 2606, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anthropic: files API\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "file = client.beta.files.upload(\n",
    "    file=(\"document.pdf\", open(\"/Users/tanushreesharma/Documents/Projects/ls-tracing-testing/python/document.pdf\", \"rb\"), \"application/pdf\"),\n",
    ")\n",
    "pdf_file_id = file.id\n",
    "\n",
    "model_anthropic_content_blocks_files = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", output_version=\"v1\", thinking=thinking, betas=[\"files-api-2025-04-14\"])\n",
    "\n",
    "input_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe this document.\"},\n",
    "        {\"type\": \"file\", \"file_id\": pdf_file_id}\n",
    "    ],\n",
    "}\n",
    "model_anthropic_content_blocks_files.invoke([input_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082223e7",
   "metadata": {},
   "source": [
    "## this might be an OSS thing, but ideally we trace the file too\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=22d55efa-c44f-4459-b398-7fb30df66dfc&peeked_trace=22d55efa-c44f-4459-b398-7fb30df66dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70fe5841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'rs_0d9cf690af3d23200068ed7df76510819ca2c9b69a152f9d8e',\n",
       "  'type': 'reasoning',\n",
       "  'reasoning': \"**Clarifying color question**\\n\\nThe user asks about the colors of grass and sky, but it's a bit ambiguous. I'm wondering if they mean specific grass and sky or just a general observation. They might have uploaded an image, but their text doesn’t mention it directly. If it is an image, I should analyze it. For now, I think it’s best to give a general answer: grass is typically green, and the sky is blue during the daytime.\"},\n",
       " {'id': 'rs_0d9cf690af3d23200068ed7df76510819ca2c9b69a152f9d8e',\n",
       "  'type': 'reasoning',\n",
       "  'reasoning': \"**Providing color context**\\n\\nI also want to mention that grass can vary in color during dawn or dusk and might appear gray in overcast conditions. However, the user's question feels straightforward, so I’ll keep it concise: grass is generally green, and the sky is blue, assuming it’s daytime. It’s good to include that colors can change with sunrise, sunset, or clouds. I should also offer to describe any colors if the user has an image, even though none is specified.\"},\n",
       " {'id': 'rs_0d9cf690af3d23200068ed7df76510819ca2c9b69a152f9d8e',\n",
       "  'type': 'reasoning',\n",
       "  'reasoning': '**Crafting a concise response**\\n\\nI think it’s worth noting that grass can appear yellowish or brown if it’s dry. However, I want to keep it brief. So, I\\'ll respond that typically, grass is green and the sky is blue on a clear day. If it’s cloudy or during dawn or dusk, the colors can change. I’ll include that if the user has an image, I can describe its colors, but for now, I\\'ll stick with: \"Grass is green and the sky is blue.\"'},\n",
       " {'type': 'text',\n",
       "  'text': 'Grass is green and the sky is blue (on a clear day). Colors can vary with clouds, sunrise/sunset, or if the grass is dry (yellow/brown) or the sky is gray. If you have a specific image, I can describe its colors precisely.',\n",
       "  'annotations': [],\n",
       "  'id': 'msg_0d9cf690af3d23200068ed7e01a744819c9838a6dcc481e684'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anthropic: citations\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"document\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"media_type\": \"text/plain\",\n",
    "                    \"data\": \"The grass is green. The sky is blue.\",\n",
    "                },\n",
    "                \"title\": \"My Document\",\n",
    "                \"context\": \"This is a trustworthy document.\",\n",
    "                \"citations\": {\"enabled\": True},\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "response = model_oai_content_blocks.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "542b75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anthropic: web search \n",
    "\n",
    "tool = {\"type\": \"web_search_20250305\", \"name\": \"web_search\", \"max_uses\": 3}\n",
    "llm_with_tools = model_oai_content_blocks.bind_tools([tool])\n",
    "\n",
    "response = model_anthropic_content_blocks.invoke(\"How do I update a web app to TypeScript 5.5?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8aaed8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anthropic: fetch\n",
    "model_anthropic_content_blocks_web_fetch = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", output_version=\"v1\", thinking=thinking, betas=[\"web-fetch-2025-09-10\"])\n",
    "\n",
    "tool = {\"type\": \"web_fetch_20250910\", \"name\": \"web_fetch\", \"max_uses\": 3}\n",
    "llm_with_tools = model_anthropic_content_blocks_web_fetch.bind_tools([tool])\n",
    "\n",
    "response = llm_with_tools.invoke(\n",
    "    \"Please analyze the content at https://example.com/article\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e4986",
   "metadata": {},
   "source": [
    "### didnt test memory tool, text editor\n",
    "\n",
    "remote MCP: tested in wrap_anthropic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0e7d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': [{'token': 'Par', 'bytes': [80, 97, 114], 'logprob': -7.896309739408025e-07, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -1.1472419600977446e-06, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.1283116638660431, 'top_logprobs': []}, {'token': ' known', 'bytes': [32, 107, 110, 111, 119, 110], 'logprob': -0.04957265406847, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.0021852378267794847, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -2.1008713702030946e-06, 'top_logprobs': []}, {'token': ' ability', 'bytes': [32, 97, 98, 105, 108, 105, 116, 121], 'logprob': -0.008262669667601585, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' mimic', 'bytes': [32, 109, 105, 109, 105, 99], 'logprob': -0.009718899615108967, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.11510754376649857, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -1.4498052223643754e-05, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.31478723883628845, 'top_logprobs': []}, {'token': ' other', 'bytes': [32, 111, 116, 104, 101, 114], 'logprob': -0.04102012515068054, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.012663504108786583, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.1269150823354721, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -1.8009867668151855, 'top_logprobs': []}, {'token': ' this', 'bytes': [32, 116, 104, 105, 115], 'logprob': -0.2451019585132599, 'top_logprobs': []}, {'token': ' ability', 'bytes': [32, 97, 98, 105, 108, 105, 116, 121], 'logprob': -0.969874918460846, 'top_logprobs': []}, {'token': ' stems', 'bytes': [32, 115, 116, 101, 109, 115], 'logprob': -2.732077121734619, 'top_logprobs': []}, {'token': ' from', 'bytes': [32, 102, 114, 111, 109], 'logprob': -0.006410650908946991, 'top_logprobs': []}, {'token': ' several', 'bytes': [32, 115, 101, 118, 101, 114, 97, 108], 'logprob': -0.457074373960495, 'top_logprobs': []}, {'token': ' factors', 'bytes': [32, 102, 97, 99, 116, 111, 114, 115], 'logprob': -0.1107272282242775, 'top_logprobs': []}, {'token': ':\\n\\n', 'bytes': [58, 10, 10], 'logprob': -0.1683921366930008, 'top_logprobs': []}, {'token': '1', 'bytes': [49], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -6.630610641877865e-06, 'top_logprobs': []}, {'token': 'V', 'bytes': [86], 'logprob': -0.27183622121810913, 'top_logprobs': []}, {'token': 'ocal', 'bytes': [111, 99, 97, 108], 'logprob': -1.1472419600977446e-06, 'top_logprobs': []}, {'token': ' Mim', 'bytes': [32, 77, 105, 109], 'logprob': -3.3048694133758545, 'top_logprobs': []}, {'token': 'ic', 'bytes': [105, 99], 'logprob': -0.0006690711015835404, 'top_logprobs': []}, {'token': 'ry', 'bytes': [114, 121], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -0.3716341555118561, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -2.1888679839321412e-05, 'top_logprobs': []}, {'token': ' Par', 'bytes': [32, 80, 97, 114], 'logprob': -0.004425251390784979, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -3.4121114822482923e-06, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.8702284693717957, 'top_logprobs': []}, {'token': ' excellent', 'bytes': [32, 101, 120, 99, 101, 108, 108, 101, 110, 116], 'logprob': -1.0336697101593018, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -0.02239009365439415, 'top_logprobs': []}, {'token': ' mim', 'bytes': [32, 109, 105, 109], 'logprob': -0.06273288279771805, 'top_logprobs': []}, {'token': 'ics', 'bytes': [105, 99, 115], 'logprob': -0.00017994173686020076, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -1.369809865951538, 'top_logprobs': []}, {'token': ' which', 'bytes': [32, 119, 104, 105, 99, 104], 'logprob': -0.5024283528327942, 'top_logprobs': []}, {'token': ' means', 'bytes': [32, 109, 101, 97, 110, 115], 'logprob': -0.3113367259502411, 'top_logprobs': []}, {'token': ' they', 'bytes': [32, 116, 104, 101, 121], 'logprob': -0.003892830340191722, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -0.1478862762451172, 'top_logprobs': []}, {'token': ' imitate', 'bytes': [32, 105, 109, 105, 116, 97, 116, 101], 'logprob': -0.41306573152542114, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -1.7412515878677368, 'top_logprobs': []}, {'token': ' they', 'bytes': [32, 116, 104, 101, 121], 'logprob': -0.10816726088523865, 'top_logprobs': []}, {'token': ' hear', 'bytes': [32, 104, 101, 97, 114], 'logprob': -0.0004658233083318919, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -0.3118041157722473, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -3.5954712075181305e-05, 'top_logprobs': []}, {'token': ' environment', 'bytes': [32, 101, 110, 118, 105, 114, 111, 110, 109, 101, 110, 116], 'logprob': -0.0012365375878289342, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.06211366876959801, 'top_logprobs': []}, {'token': ' This', 'bytes': [32, 84, 104, 105, 115], 'logprob': -0.13207250833511353, 'top_logprobs': []}, {'token': ' skill', 'bytes': [32, 115, 107, 105, 108, 108], 'logprob': -1.4740612506866455, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -0.15028807520866394, 'top_logprobs': []}, {'token': ' not', 'bytes': [32, 110, 111, 116], 'logprob': -0.9729272127151489, 'top_logprobs': []}, {'token': ' limited', 'bytes': [32, 108, 105, 109, 105, 116, 101, 100], 'logprob': -0.8659029006958008, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.0004098195640835911, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.012827933765947819, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.04492780193686485, 'top_logprobs': []}, {'token': ' but', 'bytes': [32, 98, 117, 116], 'logprob': -1.084885597229004, 'top_logprobs': []}, {'token': ' extends', 'bytes': [32, 101, 120, 116, 101, 110, 100, 115], 'logprob': -1.1915972232818604, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -9.088346359931165e-07, 'top_logprobs': []}, {'token': ' various', 'bytes': [32, 118, 97, 114, 105, 111, 117, 115], 'logprob': -1.6031944751739502, 'top_logprobs': []}, {'token': ' noises', 'bytes': [32, 110, 111, 105, 115, 101, 115], 'logprob': -1.0489816665649414, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.46929049491882324, 'top_logprobs': []}, {'token': ' like', 'bytes': [32, 108, 105, 107, 101], 'logprob': -2.54140567779541, 'top_logprobs': []}, {'token': ' other', 'bytes': [32, 111, 116, 104, 101, 114], 'logprob': -0.9524132609367371, 'top_logprobs': []}, {'token': ' animals', 'bytes': [32, 97, 110, 105, 109, 97, 108, 115], 'logprob': -0.9317807555198669, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -0.7932111024856567, 'top_logprobs': []}, {'token': ' even', 'bytes': [32, 101, 118, 101, 110], 'logprob': -2.0245954990386963, 'top_logprobs': []}, {'token': ' mechanical', 'bytes': [32, 109, 101, 99, 104, 97, 110, 105, 99, 97, 108], 'logprob': -0.18605589866638184, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.0038415249437093735, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.14592595398426056, 'top_logprobs': []}, {'token': '2', 'bytes': [50], 'logprob': 0.0, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': 'Social', 'bytes': [83, 111, 99, 105, 97, 108], 'logprob': -0.031045442447066307, 'top_logprobs': []}, {'token': ' Nature', 'bytes': [32, 78, 97, 116, 117, 114, 101], 'logprob': -1.1075036525726318, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -4.36574100604048e-06, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' Par', 'bytes': [32, 80, 97, 114], 'logprob': -0.04438871145248413, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -1.3856492842023727e-06, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.00032831361750140786, 'top_logprobs': []}, {'token': ' highly', 'bytes': [32, 104, 105, 103, 104, 108, 121], 'logprob': -0.08674337714910507, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -0.00023965541913639754, 'top_logprobs': []}, {'token': ' animals', 'bytes': [32, 97, 110, 105, 109, 97, 108, 115], 'logprob': -0.31959038972854614, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.6201866865158081, 'top_logprobs': []}, {'token': ' In', 'bytes': [32, 73, 110], 'logprob': -0.00022690063633490354, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -0.0013542831875383854, 'top_logprobs': []}, {'token': ' wild', 'bytes': [32, 119, 105, 108, 100], 'logprob': 0.0, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -4.320199877838604e-07, 'top_logprobs': []}, {'token': ' they', 'bytes': [32, 116, 104, 101, 121], 'logprob': -0.04745867848396301, 'top_logprobs': []}, {'token': ' live', 'bytes': [32, 108, 105, 118, 101], 'logprob': -0.7231537699699402, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -5.717296517104842e-05, 'top_logprobs': []}, {'token': ' fl', 'bytes': [32, 102, 108], 'logprob': -0.08295439183712006, 'top_logprobs': []}, {'token': 'ocks', 'bytes': [111, 99, 107, 115], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' where', 'bytes': [32, 119, 104, 101, 114, 101], 'logprob': -2.1051416397094727, 'top_logprobs': []}, {'token': ' communication', 'bytes': [32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110], 'logprob': -0.14952583611011505, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -0.05316365510225296, 'top_logprobs': []}, {'token': ' crucial', 'bytes': [32, 99, 114, 117, 99, 105, 97, 108], 'logprob': -0.6319595575332642, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.0622987225651741, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -1.0614291429519653, 'top_logprobs': []}, {'token': ' bonding', 'bytes': [32, 98, 111, 110, 100, 105, 110, 103], 'logprob': -0.2339605987071991, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.9007275700569153, 'top_logprobs': []}, {'token': ' coordination', 'bytes': [32, 99, 111, 111, 114, 100, 105, 110, 97, 116, 105, 111, 110], 'logprob': -1.2244271039962769, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.003957886248826981, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.00179203599691391, 'top_logprobs': []}, {'token': ' survival', 'bytes': [32, 115, 117, 114, 118, 105, 118, 97, 108], 'logprob': -0.03624087572097778, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.00017565040616318583, 'top_logprobs': []}, {'token': ' Mim', 'bytes': [32, 77, 105, 109], 'logprob': -0.21158942580223083, 'top_logprobs': []}, {'token': 'icking', 'bytes': [105, 99, 107, 105, 110, 103], 'logprob': -0.006727095227688551, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.15418104827404022, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.3929761052131653, 'top_logprobs': []}, {'token': ' including', 'bytes': [32, 105, 110, 99, 108, 117, 100, 105, 110, 103], 'logprob': -0.0015730679733678699, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -2.5016274452209473, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.0052974349819123745, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -0.9754197597503662, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.2625367045402527, 'top_logprobs': []}, {'token': ' way', 'bytes': [32, 119, 97, 121], 'logprob': -0.07577918469905853, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.08195643126964569, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.9957007169723511, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.04007035493850708, 'top_logprobs': []}, {'token': ' integrate', 'bytes': [32, 105, 110, 116, 101, 103, 114, 97, 116, 101], 'logprob': -4.149360656738281, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -1.058998465538025, 'top_logprobs': []}, {'token': ' interact', 'bytes': [32, 105, 110, 116, 101, 114, 97, 99, 116], 'logprob': -0.7853170037269592, 'top_logprobs': []}, {'token': ' with', 'bytes': [32, 119, 105, 116, 104], 'logprob': -0.180702805519104, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.05514833331108093, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.6129066944122314, 'top_logprobs': []}, {'token': ' \"', 'bytes': [32, 34], 'logprob': -0.17676539719104767, 'top_logprobs': []}, {'token': 'fl', 'bytes': [102, 108], 'logprob': -1.0087516784551553e-05, 'top_logprobs': []}, {'token': 'ock', 'bytes': [111, 99, 107], 'logprob': -0.023256074637174606, 'top_logprobs': []}, {'token': '\"', 'bytes': [34], 'logprob': -0.4914672076702118, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -1.067897915840149, 'top_logprobs': []}, {'token': ' captivity', 'bytes': [32, 99, 97, 112, 116, 105, 118, 105, 116, 121], 'logprob': -0.45740482211112976, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.01460837572813034, 'top_logprobs': []}, {'token': '3', 'bytes': [51], 'logprob': 0.0, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -5.512236498361744e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': 'C', 'bytes': [67], 'logprob': -1.0083117485046387, 'top_logprobs': []}, {'token': 'ognitive', 'bytes': [111, 103, 110, 105, 116, 105, 118, 101], 'logprob': -0.04999922960996628, 'top_logprobs': []}, {'token': ' Ability', 'bytes': [32, 65, 98, 105, 108, 105, 116, 121], 'logprob': -2.4655301570892334, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -4.4849443838757e-06, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' Par', 'bytes': [32, 80, 97, 114], 'logprob': -0.029675627127289772, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -6.749814019713085e-06, 'top_logprobs': []}, {'token': ' have', 'bytes': [32, 104, 97, 118, 101], 'logprob': -0.642250120639801, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -2.0274364948272705, 'top_logprobs': []}, {'token': ' high', 'bytes': [32, 104, 105, 103, 104], 'logprob': -0.5968925952911377, 'top_logprobs': []}, {'token': ' level', 'bytes': [32, 108, 101, 118, 101, 108], 'logprob': -0.015395049005746841, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' intelligence', 'bytes': [32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101], 'logprob': -0.0052782269194722176, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.9356870055198669, 'top_logprobs': []}, {'token': ' which', 'bytes': [32, 119, 104, 105, 99, 104], 'logprob': -0.5976507663726807, 'top_logprobs': []}, {'token': ' allows', 'bytes': [32, 97, 108, 108, 111, 119, 115], 'logprob': -1.4580514430999756, 'top_logprobs': []}, {'token': ' them', 'bytes': [32, 116, 104, 101, 109], 'logprob': -0.0005014431662857533, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.010306783020496368, 'top_logprobs': []}, {'token': ' learn', 'bytes': [32, 108, 101, 97, 114, 110], 'logprob': -0.9664394855499268, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.08179562538862228, 'top_logprobs': []}, {'token': ' replicate', 'bytes': [32, 114, 101, 112, 108, 105, 99, 97, 116, 101], 'logprob': -0.9916850328445435, 'top_logprobs': []}, {'token': ' complex', 'bytes': [32, 99, 111, 109, 112, 108, 101, 120], 'logprob': -0.09609760344028473, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.17260593175888062, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.02772808074951172, 'top_logprobs': []}, {'token': ' Some', 'bytes': [32, 83, 111, 109, 101], 'logprob': -1.610011339187622, 'top_logprobs': []}, {'token': ' studies', 'bytes': [32, 115, 116, 117, 100, 105, 101, 115], 'logprob': -2.0598058700561523, 'top_logprobs': []}, {'token': ' suggest', 'bytes': [32, 115, 117, 103, 103, 101, 115, 116], 'logprob': -0.06475943326950073, 'top_logprobs': []}, {'token': ' that', 'bytes': [32, 116, 104, 97, 116], 'logprob': -0.11341933906078339, 'top_logprobs': []}, {'token': ' talking', 'bytes': [32, 116, 97, 108, 107, 105, 110, 103], 'logprob': -3.6755127906799316, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -1.3420209884643555, 'top_logprobs': []}, {'token': ' mim', 'bytes': [32, 109, 105, 109], 'logprob': -0.46159228682518005, 'top_logprobs': []}, {'token': 'icking', 'bytes': [105, 99, 107, 105, 110, 103], 'logprob': -4.56102097814437e-05, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -1.1936436891555786, 'top_logprobs': []}, {'token': ' could', 'bytes': [32, 99, 111, 117, 108, 100], 'logprob': -2.117799758911133, 'top_logprobs': []}, {'token': ' be', 'bytes': [32, 98, 101], 'logprob': -0.3166311979293823, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.104840949177742, 'top_logprobs': []}, {'token': ' way', 'bytes': [32, 119, 97, 121], 'logprob': -1.1535290479660034, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.1279660016298294, 'top_logprobs': []}, {'token': ' them', 'bytes': [32, 116, 104, 101, 109], 'logprob': -1.3208969831466675, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -1.2352385965641588e-05, 'top_logprobs': []}, {'token': ' exercise', 'bytes': [32, 101, 120, 101, 114, 99, 105, 115, 101], 'logprob': -1.7345235347747803, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.015391054563224316, 'top_logprobs': []}, {'token': ' cognitive', 'bytes': [32, 99, 111, 103, 110, 105, 116, 105, 118, 101], 'logprob': -0.03785865753889084, 'top_logprobs': []}, {'token': ' skills', 'bytes': [32, 115, 107, 105, 108, 108, 115], 'logprob': -1.3433163166046143, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.14776504039764404, 'top_logprobs': []}, {'token': '4', 'bytes': [52], 'logprob': -8.061054359131958e-06, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -7.896309739408025e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -2.2200749754119897e-06, 'top_logprobs': []}, {'token': 'Attention', 'bytes': [65, 116, 116, 101, 110, 116, 105, 111, 110], 'logprob': -0.09826718270778656, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.29353341460227966, 'top_logprobs': []}, {'token': ' St', 'bytes': [32, 83, 116], 'logprob': -1.8653708696365356, 'top_logprobs': []}, {'token': 'imulation', 'bytes': [105, 109, 117, 108, 97, 116, 105, 111, 110], 'logprob': -2.696889623621246e-06, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -6.587483221665025e-05, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' In', 'bytes': [32, 73, 110], 'logprob': -0.9120258092880249, 'top_logprobs': []}, {'token': ' captivity', 'bytes': [32, 99, 97, 112, 116, 105, 118, 105, 116, 121], 'logprob': -0.4016452133655548, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -7.822646693966817e-06, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.007414243184030056, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': -2.5776860184123507e-06, 'top_logprobs': []}, {'token': ' often', 'bytes': [32, 111, 102, 116, 101, 110], 'logprob': -0.579185962677002, 'top_logprobs': []}, {'token': ' mimic', 'bytes': [32, 109, 105, 109, 105, 99], 'logprob': -0.5764991641044617, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.22652193903923035, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.007408681325614452, 'top_logprobs': []}, {'token': ' because', 'bytes': [32, 98, 101, 99, 97, 117, 115, 101], 'logprob': -0.7325064539909363, 'top_logprobs': []}, {'token': ' it', 'bytes': [32, 105, 116], 'logprob': -0.39358964562416077, 'top_logprobs': []}, {'token': ' garn', 'bytes': [32, 103, 97, 114, 110], 'logprob': -0.68031245470047, 'top_logprobs': []}, {'token': 'ers', 'bytes': [101, 114, 115], 'logprob': -1.306760805164231e-05, 'top_logprobs': []}, {'token': ' attention', 'bytes': [32, 97, 116, 116, 101, 110, 116, 105, 111, 110], 'logprob': -0.06913789361715317, 'top_logprobs': []}, {'token': ' from', 'bytes': [32, 102, 114, 111, 109], 'logprob': -0.5335959196090698, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.03503379598259926, 'top_logprobs': []}, {'token': ' owners', 'bytes': [32, 111, 119, 110, 101, 114, 115], 'logprob': -0.4860728085041046, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.2528383135795593, 'top_logprobs': []}, {'token': ' Eng', 'bytes': [32, 69, 110, 103], 'logprob': -3.2235119342803955, 'top_logprobs': []}, {'token': 'aging', 'bytes': [97, 103, 105, 110, 103], 'logprob': -0.0019383925246074796, 'top_logprobs': []}, {'token': ' with', 'bytes': [32, 119, 105, 116, 104], 'logprob': -0.3339473307132721, 'top_logprobs': []}, {'token': ' humans', 'bytes': [32, 104, 117, 109, 97, 110, 115], 'logprob': -0.49214300513267517, 'top_logprobs': []}, {'token': ' by', 'bytes': [32, 98, 121], 'logprob': -1.4809612035751343, 'top_logprobs': []}, {'token': ' mim', 'bytes': [32, 109, 105, 109], 'logprob': -1.159845232963562, 'top_logprobs': []}, {'token': 'icking', 'bytes': [105, 99, 107, 105, 110, 103], 'logprob': -4.3464544432936236e-05, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.357696533203125, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -1.0882195234298706, 'top_logprobs': []}, {'token': ' patterns', 'bytes': [32, 112, 97, 116, 116, 101, 114, 110, 115], 'logprob': -6.40974235534668, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -0.23395229876041412, 'top_logprobs': []}, {'token': ' also', 'bytes': [32, 97, 108, 115, 111], 'logprob': -2.013303518295288, 'top_logprobs': []}, {'token': ' provide', 'bytes': [32, 112, 114, 111, 118, 105, 100, 101], 'logprob': -0.5573304891586304, 'top_logprobs': []}, {'token': ' mental', 'bytes': [32, 109, 101, 110, 116, 97, 108], 'logprob': -0.09214664250612259, 'top_logprobs': []}, {'token': ' stimulation', 'bytes': [32, 115, 116, 105, 109, 117, 108, 97, 116, 105, 111, 110], 'logprob': -0.005067602731287479, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.9521873593330383, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -1.0936675071716309, 'top_logprobs': []}, {'token': ' birds', 'bytes': [32, 98, 105, 114, 100, 115], 'logprob': -1.247096300125122, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.4349401891231537, 'top_logprobs': []}, {'token': '5', 'bytes': [53], 'logprob': -0.11703383177518845, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -6.704273118884885e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -7.822646693966817e-06, 'top_logprobs': []}, {'token': 'Natural', 'bytes': [78, 97, 116, 117, 114, 97, 108], 'logprob': -3.4107275009155273, 'top_logprobs': []}, {'token': ' Inst', 'bytes': [32, 73, 110, 115, 116], 'logprob': -1.562563180923462, 'top_logprobs': []}, {'token': 'inct', 'bytes': [105, 110, 99, 116], 'logprob': -1.306760805164231e-05, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -1.0645346641540527, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -5.9153885558771435e-06, 'top_logprobs': []}, {'token': ' In', 'bytes': [32, 73, 110], 'logprob': -0.3255361020565033, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -0.729908287525177, 'top_logprobs': []}, {'token': ' wild', 'bytes': [32, 119, 105, 108, 100], 'logprob': -0.0006080791936255991, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -1.6286106983898208e-05, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -2.695591449737549, 'top_logprobs': []}, {'token': ' mimic', 'bytes': [32, 109, 105, 109, 105, 99], 'logprob': -1.1365090608596802, 'top_logprobs': []}, {'token': 'ry', 'bytes': [114, 121], 'logprob': -4.320199877838604e-07, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -0.5370392799377441, 'top_logprobs': []}, {'token': ' have', 'bytes': [32, 104, 97, 118, 101], 'logprob': -2.53330659866333, 'top_logprobs': []}, {'token': ' practical', 'bytes': [32, 112, 114, 97, 99, 116, 105, 99, 97, 108], 'logprob': -0.32735517621040344, 'top_logprobs': []}, {'token': ' benefits', 'bytes': [32, 98, 101, 110, 101, 102, 105, 116, 115], 'logprob': -1.4212411642074585, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.13268516957759857, 'top_logprobs': []}, {'token': ' such', 'bytes': [32, 115, 117, 99, 104], 'logprob': -0.0799454003572464, 'top_logprobs': []}, {'token': ' as', 'bytes': [32, 97, 115], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' det', 'bytes': [32, 100, 101, 116], 'logprob': -0.41854462027549744, 'top_logprobs': []}, {'token': 'erring', 'bytes': [101, 114, 114, 105, 110, 103], 'logprob': -1.1160349458805285e-05, 'top_logprobs': []}, {'token': ' predators', 'bytes': [32, 112, 114, 101, 100, 97, 116, 111, 114, 115], 'logprob': -0.003532367292791605, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -0.5612081289291382, 'top_logprobs': []}, {'token': ' attracting', 'bytes': [32, 97, 116, 116, 114, 97, 99, 116, 105, 110, 103], 'logprob': -0.43485772609710693, 'top_logprobs': []}, {'token': ' mates', 'bytes': [32, 109, 97, 116, 101, 115], 'logprob': -0.03560617193579674, 'top_logprobs': []}, {'token': ' through', 'bytes': [32, 116, 104, 114, 111, 117, 103, 104], 'logprob': -5.019699573516846, 'top_logprobs': []}, {'token': ' complex', 'bytes': [32, 99, 111, 109, 112, 108, 101, 120], 'logprob': -0.7929362654685974, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -0.9823418855667114, 'top_logprobs': []}, {'token': ' displays', 'bytes': [32, 100, 105, 115, 112, 108, 97, 121, 115], 'logprob': -0.400214821100235, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.8816351294517517, 'top_logprobs': []}, {'token': ' This', 'bytes': [32, 84, 104, 105, 115], 'logprob': -1.2655783891677856, 'top_logprobs': []}, {'token': ' natural', 'bytes': [32, 110, 97, 116, 117, 114, 97, 108], 'logprob': -0.8966162204742432, 'top_logprobs': []}, {'token': ' instinct', 'bytes': [32, 105, 110, 115, 116, 105, 110, 99, 116], 'logprob': -0.19881612062454224, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -2.238703727722168, 'top_logprobs': []}, {'token': ' manifest', 'bytes': [32, 109, 97, 110, 105, 102, 101, 115, 116], 'logprob': -1.5279349088668823, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -0.8332895636558533, 'top_logprobs': []}, {'token': ' captivity', 'bytes': [32, 99, 97, 112, 116, 105, 118, 105, 116, 121], 'logprob': -0.4159516990184784, 'top_logprobs': []}, {'token': ' as', 'bytes': [32, 97, 115], 'logprob': -0.07526906579732895, 'top_logprobs': []}, {'token': ' mim', 'bytes': [32, 109, 105, 109], 'logprob': -1.8413987159729004, 'top_logprobs': []}, {'token': 'icking', 'bytes': [105, 99, 107, 105, 110, 103], 'logprob': -0.0019473218126222491, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.18841466307640076, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -1.991105318069458, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.08097967505455017, 'top_logprobs': []}, {'token': 'Overall', 'bytes': [79, 118, 101, 114, 97, 108, 108], 'logprob': -0.35465818643569946, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.8564973473548889, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': -6.392202976712724e-06, 'top_logprobs': []}, {'token': ' talk', 'bytes': [32, 116, 97, 108, 107], 'logprob': -0.0907890722155571, 'top_logprobs': []}, {'token': ' primarily', 'bytes': [32, 112, 114, 105, 109, 97, 114, 105, 108, 121], 'logprob': -2.0026590824127197, 'top_logprobs': []}, {'token': ' as', 'bytes': [32, 97, 115], 'logprob': -2.2599618434906006, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.07236336171627045, 'top_logprobs': []}, {'token': ' form', 'bytes': [32, 102, 111, 114, 109], 'logprob': -0.8970531225204468, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -2.935296834039036e-06, 'top_logprobs': []}, {'token': ' communication', 'bytes': [32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110], 'logprob': -2.3806324005126953, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.37801262736320496, 'top_logprobs': []}, {'token': ' interaction', 'bytes': [32, 105, 110, 116, 101, 114, 97, 99, 116, 105, 111, 110], 'logprob': -0.8718460202217102, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.49358153343200684, 'top_logprobs': []}, {'token': ' both', 'bytes': [32, 98, 111, 116, 104], 'logprob': -0.8762392401695251, 'top_logprobs': []}, {'token': ' with', 'bytes': [32, 119, 105, 116, 104], 'logprob': -0.6148154735565186, 'top_logprobs': []}, {'token': ' humans', 'bytes': [32, 104, 117, 109, 97, 110, 115], 'logprob': -1.8706004619598389, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.04078404977917671, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -1.7488287687301636, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.25758421421051025, 'top_logprobs': []}, {'token': ' natural', 'bytes': [32, 110, 97, 116, 117, 114, 97, 108], 'logprob': -0.07659605145454407, 'top_logprobs': []}, {'token': ' environments', 'bytes': [32, 101, 110, 118, 105, 114, 111, 110, 109, 101, 110, 116, 115], 'logprob': -1.0020772218704224, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.023151379078626633, 'top_logprobs': []}], 'refusal': None}\n",
      "{'content': [{'token': 'Par', 'bytes': [80, 97, 114], 'logprob': -5.512236498361744e-07, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -6.704273118884885e-07, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.10834986716508865, 'top_logprobs': []}, {'token': ' known', 'bytes': [32, 107, 110, 111, 119, 110], 'logprob': -0.055876392871141434, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.0009115827269852161, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -6.704273118884885e-07, 'top_logprobs': []}, {'token': ' ability', 'bytes': [32, 97, 98, 105, 108, 105, 116, 121], 'logprob': -0.0072861965745687485, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' imitate', 'bytes': [32, 105, 109, 105, 116, 97, 116, 101], 'logprob': -5.258522987365723, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -1.608716607093811, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.14298197627067566, 'top_logprobs': []}, {'token': ' including', 'bytes': [32, 105, 110, 99, 108, 117, 100, 105, 110, 103], 'logprob': -0.002207593061029911, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.0002060436672763899, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.22544631361961365, 'top_logprobs': []}, {'token': ' primarily', 'bytes': [32, 112, 114, 105, 109, 97, 114, 105, 108, 121], 'logprob': -2.2719790935516357, 'top_logprobs': []}, {'token': ' due', 'bytes': [32, 100, 117, 101], 'logprob': -0.21467939019203186, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -4.320199877838604e-07, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.07804755121469498, 'top_logprobs': []}, {'token': ' highly', 'bytes': [32, 104, 105, 103, 104, 108, 121], 'logprob': -0.8100870251655579, 'top_logprobs': []}, {'token': ' developed', 'bytes': [32, 100, 101, 118, 101, 108, 111, 112, 101, 100], 'logprob': -0.015206863172352314, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -0.006299891974776983, 'top_logprobs': []}, {'token': ' organs', 'bytes': [32, 111, 114, 103, 97, 110, 115], 'logprob': -0.5398392081260681, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.19248782098293304, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -0.4877983629703522, 'top_logprobs': []}, {'token': ' nature', 'bytes': [32, 110, 97, 116, 117, 114, 101], 'logprob': -0.009839902631938457, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.0012386832386255264, 'top_logprobs': []}, {'token': ' There', 'bytes': [32, 84, 104, 101, 114, 101], 'logprob': -2.994316816329956, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -6.2729995988775045e-06, 'top_logprobs': []}, {'token': ' several', 'bytes': [32, 115, 101, 118, 101, 114, 97, 108], 'logprob': -0.34846705198287964, 'top_logprobs': []}, {'token': ' reasons', 'bytes': [32, 114, 101, 97, 115, 111, 110, 115], 'logprob': -0.004971051122993231, 'top_logprobs': []}, {'token': ' why', 'bytes': [32, 119, 104, 121], 'logprob': -0.015274128876626492, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.008718223311007023, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' engage', 'bytes': [32, 101, 110, 103, 97, 103, 101], 'logprob': -3.186640739440918, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -2.4883875846862793, 'top_logprobs': []}, {'token': ' mimic', 'bytes': [32, 109, 105, 109, 105, 99], 'logprob': -0.0036997238639742136, 'top_logprobs': []}, {'token': 'ry', 'bytes': [114, 121], 'logprob': 0.0, 'top_logprobs': []}, {'token': ':\\n\\n', 'bytes': [58, 10, 10], 'logprob': -0.006204884499311447, 'top_logprobs': []}, {'token': '1', 'bytes': [49], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -3.292907877039397e-06, 'top_logprobs': []}, {'token': 'Social', 'bytes': [83, 111, 99, 105, 97, 108], 'logprob': -0.047072313725948334, 'top_logprobs': []}, {'token': ' Interaction', 'bytes': [32, 73, 110, 116, 101, 114, 97, 99, 116, 105, 111, 110], 'logprob': -0.006892497185617685, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -0.11316248774528503, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -4.246537173457909e-06, 'top_logprobs': []}, {'token': ' Par', 'bytes': [32, 80, 97, 114], 'logprob': -0.3486661911010742, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -1.0280383548888494e-06, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.0011984437005594373, 'top_logprobs': []}, {'token': ' highly', 'bytes': [32, 104, 105, 103, 104, 108, 121], 'logprob': -0.45859766006469727, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -0.0009431460639461875, 'top_logprobs': []}, {'token': ' animals', 'bytes': [32, 97, 110, 105, 109, 97, 108, 115], 'logprob': -0.08677301555871964, 'top_logprobs': []}, {'token': ' that', 'bytes': [32, 116, 104, 97, 116], 'logprob': -0.8071656823158264, 'top_logprobs': []}, {'token': ' live', 'bytes': [32, 108, 105, 118, 101], 'logprob': -0.5410915017127991, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -9.729906196298543e-06, 'top_logprobs': []}, {'token': ' fl', 'bytes': [32, 102, 108], 'logprob': -0.227391317486763, 'top_logprobs': []}, {'token': 'ocks', 'bytes': [111, 99, 107, 115], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -0.2934667468070984, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -0.00023095356300473213, 'top_logprobs': []}, {'token': ' wild', 'bytes': [32, 119, 105, 108, 100], 'logprob': 0.0, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.006007717456668615, 'top_logprobs': []}, {'token': ' Vocal', 'bytes': [32, 86, 111, 99, 97, 108], 'logprob': -0.9879902005195618, 'top_logprobs': []}, {'token': ' communication', 'bytes': [32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 105, 111, 110], 'logprob': -0.5843394994735718, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -0.10062576085329056, 'top_logprobs': []}, {'token': ' crucial', 'bytes': [32, 99, 114, 117, 99, 105, 97, 108], 'logprob': -0.8771432042121887, 'top_logprobs': []}, {'token': ' for', 'bytes': [32, 102, 111, 114], 'logprob': -0.0023745812941342592, 'top_logprobs': []}, {'token': ' maintaining', 'bytes': [32, 109, 97, 105, 110, 116, 97, 105, 110, 105, 110, 103], 'logprob': -0.09318498522043228, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -0.08500723540782928, 'top_logprobs': []}, {'token': ' bonds', 'bytes': [32, 98, 111, 110, 100, 115], 'logprob': -0.0006249914295040071, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -3.182488441467285, 'top_logprobs': []}, {'token': ' In', 'bytes': [32, 73, 110], 'logprob': -0.6769319772720337, 'top_logprobs': []}, {'token': ' captivity', 'bytes': [32, 99, 97, 112, 116, 105, 118, 105, 116, 121], 'logprob': -0.1293749362230301, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.0007104054093360901, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.21241557598114014, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' often', 'bytes': [32, 111, 102, 116, 101, 110], 'logprob': -0.19985046982765198, 'top_logprobs': []}, {'token': ' mimic', 'bytes': [32, 109, 105, 109, 105, 99], 'logprob': -0.4736378490924835, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.0049194470047950745, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.0036173006519675255, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -1.413685917854309, 'top_logprobs': []}, {'token': ' interact', 'bytes': [32, 105, 110, 116, 101, 114, 97, 99, 116], 'logprob': -0.8516134023666382, 'top_logprobs': []}, {'token': ' with', 'bytes': [32, 119, 105, 116, 104], 'logprob': -0.2843715250492096, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.01078569795936346, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.013563579879701138, 'top_logprobs': []}, {'token': ' companions', 'bytes': [32, 99, 111, 109, 112, 97, 110, 105, 111, 110, 115], 'logprob': -1.858238935470581, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.37180450558662415, 'top_logprobs': []}, {'token': ' treating', 'bytes': [32, 116, 114, 101, 97, 116, 105, 110, 103], 'logprob': -2.074711799621582, 'top_logprobs': []}, {'token': ' them', 'bytes': [32, 116, 104, 101, 109], 'logprob': -0.014404304325580597, 'top_logprobs': []}, {'token': ' as', 'bytes': [32, 97, 115], 'logprob': -0.004122687969356775, 'top_logprobs': []}, {'token': ' surrogate', 'bytes': [32, 115, 117, 114, 114, 111, 103, 97, 116, 101], 'logprob': -8.662944793701172, 'top_logprobs': []}, {'token': ' flock', 'bytes': [32, 102, 108, 111, 99, 107], 'logprob': -0.027719147503376007, 'top_logprobs': []}, {'token': ' members', 'bytes': [32, 109, 101, 109, 98, 101, 114, 115], 'logprob': -0.0012664210516959429, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.0003886086633428931, 'top_logprobs': []}, {'token': '2', 'bytes': [50], 'logprob': 0.0, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': 'C', 'bytes': [67], 'logprob': -1.19416081905365, 'top_logprobs': []}, {'token': 'ognitive', 'bytes': [111, 103, 110, 105, 116, 105, 118, 101], 'logprob': -0.005304196383804083, 'top_logprobs': []}, {'token': ' St', 'bytes': [32, 83, 116], 'logprob': -1.1654483079910278, 'top_logprobs': []}, {'token': 'imulation', 'bytes': [105, 109, 117, 108, 97, 116, 105, 111, 110], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -3.1782583391759545e-05, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' Par', 'bytes': [32, 80, 97, 114], 'logprob': -0.019820716232061386, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -3.03521392197581e-05, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.0030158571898937225, 'top_logprobs': []}, {'token': ' intelligent', 'bytes': [32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 116], 'logprob': -0.031093403697013855, 'top_logprobs': []}, {'token': ' birds', 'bytes': [32, 98, 105, 114, 100, 115], 'logprob': -0.5006981492042542, 'top_logprobs': []}, {'token': ' that', 'bytes': [32, 116, 104, 97, 116], 'logprob': -0.50452721118927, 'top_logprobs': []}, {'token': ' require', 'bytes': [32, 114, 101, 113, 117, 105, 114, 101], 'logprob': -0.026276845484972, 'top_logprobs': []}, {'token': ' mental', 'bytes': [32, 109, 101, 110, 116, 97, 108], 'logprob': -0.0072425296530127525, 'top_logprobs': []}, {'token': ' stimulation', 'bytes': [32, 115, 116, 105, 109, 117, 108, 97, 116, 105, 111, 110], 'logprob': -0.016292884945869446, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.021823745220899582, 'top_logprobs': []}, {'token': ' Mim', 'bytes': [32, 77, 105, 109], 'logprob': -0.24503114819526672, 'top_logprobs': []}, {'token': 'icking', 'bytes': [105, 99, 107, 105, 110, 103], 'logprob': -0.002809036523103714, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.035255011171102524, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.5196408629417419, 'top_logprobs': []}, {'token': ' learning', 'bytes': [32, 108, 101, 97, 114, 110, 105, 110, 103], 'logprob': -0.7627546191215515, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -1.4315688610076904, 'top_logprobs': []}, {'token': ' \"', 'bytes': [32, 34], 'logprob': -0.25834932923316956, 'top_logprobs': []}, {'token': 'talk', 'bytes': [116, 97, 108, 107], 'logprob': -0.006715728435665369, 'top_logprobs': []}, {'token': '\"', 'bytes': [34], 'logprob': -5.0259150157216936e-05, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -3.2293527126312256, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.1943458467721939, 'top_logprobs': []}, {'token': ' form', 'bytes': [32, 102, 111, 114, 109], 'logprob': -0.6947723627090454, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -9.088346359931165e-07, 'top_logprobs': []}, {'token': ' cognitive', 'bytes': [32, 99, 111, 103, 110, 105, 116, 105, 118, 101], 'logprob': -1.244708776473999, 'top_logprobs': []}, {'token': ' enrichment', 'bytes': [32, 101, 110, 114, 105, 99, 104, 109, 101, 110, 116], 'logprob': -1.139773964881897, 'top_logprobs': []}, {'token': ' that', 'bytes': [32, 116, 104, 97, 116], 'logprob': -0.3628256022930145, 'top_logprobs': []}, {'token': ' keeps', 'bytes': [32, 107, 101, 101, 112, 115], 'logprob': -0.5590821504592896, 'top_logprobs': []}, {'token': ' them', 'bytes': [32, 116, 104, 101, 109], 'logprob': -0.14555883407592773, 'top_logprobs': []}, {'token': ' engaged', 'bytes': [32, 101, 110, 103, 97, 103, 101, 100], 'logprob': -0.11162461340427399, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.2711331844329834, 'top_logprobs': []}, {'token': ' stimulated', 'bytes': [32, 115, 116, 105, 109, 117, 108, 97, 116, 101, 100], 'logprob': -3.685608386993408, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.27907514572143555, 'top_logprobs': []}, {'token': '3', 'bytes': [51], 'logprob': 0.0, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -5.512236498361744e-07, 'top_logprobs': []}, {'token': 'Attention', 'bytes': [65, 116, 116, 101, 110, 116, 105, 111, 110], 'logprob': -0.24848507344722748, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -3.903963088989258, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -9.372294698550832e-06, 'top_logprobs': []}, {'token': ' Par', 'bytes': [32, 80, 97, 114], 'logprob': -0.09545037895441055, 'top_logprobs': []}, {'token': 'rots', 'bytes': [114, 111, 116, 115], 'logprob': -1.0087516784551553e-05, 'top_logprobs': []}, {'token': ' quickly', 'bytes': [32, 113, 117, 105, 99, 107, 108, 121], 'logprob': -0.8997454047203064, 'top_logprobs': []}, {'token': ' learn', 'bytes': [32, 108, 101, 97, 114, 110], 'logprob': -0.002183095784857869, 'top_logprobs': []}, {'token': ' that', 'bytes': [32, 116, 104, 97, 116], 'logprob': -0.004639811348170042, 'top_logprobs': []}, {'token': ' mim', 'bytes': [32, 109, 105, 109], 'logprob': -0.5884828567504883, 'top_logprobs': []}, {'token': 'icking', 'bytes': [105, 99, 107, 105, 110, 103], 'logprob': -1.9743012671824545e-05, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.20555007457733154, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.08341390639543533, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -1.3023905754089355, 'top_logprobs': []}, {'token': ' making', 'bytes': [32, 109, 97, 107, 105, 110, 103], 'logprob': -1.9828686714172363, 'top_logprobs': []}, {'token': ' certain', 'bytes': [32, 99, 101, 114, 116, 97, 105, 110], 'logprob': -0.48887890577316284, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.06204644590616226, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -0.4750399887561798, 'top_logprobs': []}, {'token': ' attract', 'bytes': [32, 97, 116, 116, 114, 97, 99, 116], 'logprob': -0.7916479110717773, 'top_logprobs': []}, {'token': ' attention', 'bytes': [32, 97, 116, 116, 101, 110, 116, 105, 111, 110], 'logprob': -0.24653217196464539, 'top_logprobs': []}, {'token': ' from', 'bytes': [32, 102, 114, 111, 109], 'logprob': -0.1366482973098755, 'top_logprobs': []}, {'token': ' humans', 'bytes': [32, 104, 117, 109, 97, 110, 115], 'logprob': -2.6067423820495605, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.1802278459072113, 'top_logprobs': []}, {'token': ' This', 'bytes': [32, 84, 104, 105, 115], 'logprob': -0.9813425540924072, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -1.395459771156311, 'top_logprobs': []}, {'token': ' be', 'bytes': [32, 98, 101], 'logprob': -0.7131152153015137, 'top_logprobs': []}, {'token': ' particularly', 'bytes': [32, 112, 97, 114, 116, 105, 99, 117, 108, 97, 114, 108, 121], 'logprob': -2.1914799213409424, 'top_logprobs': []}, {'token': ' appealing', 'bytes': [32, 97, 112, 112, 101, 97, 108, 105, 110, 103], 'logprob': -1.6577256917953491, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.7130482792854309, 'top_logprobs': []}, {'token': ' them', 'bytes': [32, 116, 104, 101, 109], 'logprob': -1.0736159086227417, 'top_logprobs': []}, {'token': ' if', 'bytes': [32, 105, 102], 'logprob': -1.0226242542266846, 'top_logprobs': []}, {'token': ' they', 'bytes': [32, 116, 104, 101, 121], 'logprob': -0.10557672381401062, 'top_logprobs': []}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -1.079628348350525, 'top_logprobs': []}, {'token': ' seeking', 'bytes': [32, 115, 101, 101, 107, 105, 110, 103], 'logprob': -0.0962933748960495, 'top_logprobs': []}, {'token': ' interaction', 'bytes': [32, 105, 110, 116, 101, 114, 97, 99, 116, 105, 111, 110], 'logprob': -0.16710472106933594, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -0.19968335330486298, 'top_logprobs': []}, {'token': ' treats', 'bytes': [32, 116, 114, 101, 97, 116, 115], 'logprob': -2.3688700199127197, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.005309181287884712, 'top_logprobs': []}, {'token': '4', 'bytes': [52], 'logprob': -5.9153885558771435e-06, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -5.512236498361744e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -2.45848218582978e-06, 'top_logprobs': []}, {'token': 'Natural', 'bytes': [78, 97, 116, 117, 114, 97, 108], 'logprob': -1.2681485414505005, 'top_logprobs': []}, {'token': ' Behavior', 'bytes': [32, 66, 101, 104, 97, 118, 105, 111, 114], 'logprob': -1.5685603618621826, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -0.00028302724240347743, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' In', 'bytes': [32, 73, 110], 'logprob': -0.04794012010097504, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -0.22606295347213745, 'top_logprobs': []}, {'token': ' wild', 'bytes': [32, 119, 105, 108, 100], 'logprob': -1.1472419600977446e-06, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -7.896309739408025e-07, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.10251490771770477, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' mimic', 'bytes': [32, 109, 105, 109, 105, 99], 'logprob': -1.2931164503097534, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -0.7613142728805542, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -1.3140463829040527, 'top_logprobs': []}, {'token': ' they', 'bytes': [32, 116, 104, 101, 121], 'logprob': -2.3070223331451416, 'top_logprobs': []}, {'token': ' hear', 'bytes': [32, 104, 101, 97, 114], 'logprob': -0.0028828566428273916, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -3.521399974822998, 'top_logprobs': []}, {'token': ' communicate', 'bytes': [32, 99, 111, 109, 109, 117, 110, 105, 99, 97, 116, 101], 'logprob': -0.36932098865509033, 'top_logprobs': []}, {'token': ' with', 'bytes': [32, 119, 105, 116, 104], 'logprob': -0.33217716217041016, 'top_logprobs': []}, {'token': ' each', 'bytes': [32, 101, 97, 99, 104], 'logprob': -1.2771847248077393, 'top_logprobs': []}, {'token': ' other', 'bytes': [32, 111, 116, 104, 101, 114], 'logprob': -1.2664456789934775e-06, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.8617488145828247, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.6165559887886047, 'top_logprobs': []}, {'token': ' fit', 'bytes': [32, 102, 105, 116], 'logprob': -1.8213304281234741, 'top_logprobs': []}, {'token': ' into', 'bytes': [32, 105, 110, 116, 111], 'logprob': -0.3491857647895813, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.0834793746471405, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -1.618746280670166, 'top_logprobs': []}, {'token': ' environment', 'bytes': [32, 101, 110, 118, 105, 114, 111, 110, 109, 101, 110, 116], 'logprob': -1.406087040901184, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.009240183979272842, 'top_logprobs': []}, {'token': ' The', 'bytes': [32, 84, 104, 101], 'logprob': -4.181667804718018, 'top_logprobs': []}, {'token': ' ability', 'bytes': [32, 97, 98, 105, 108, 105, 116, 121], 'logprob': -0.06206100806593895, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.00015419373812619597, 'top_logprobs': []}, {'token': ' imitate', 'bytes': [32, 105, 109, 105, 116, 97, 116, 101], 'logprob': -1.6110470294952393, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -1.8449897766113281, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -1.0460450649261475, 'top_logprobs': []}, {'token': ' an', 'bytes': [32, 97, 110], 'logprob': -1.209000587463379, 'top_logprobs': []}, {'token': ' evolutionary', 'bytes': [32, 101, 118, 111, 108, 117, 116, 105, 111, 110, 97, 114, 121], 'logprob': -1.7120482921600342, 'top_logprobs': []}, {'token': ' trait', 'bytes': [32, 116, 114, 97, 105, 116], 'logprob': -0.7676750421524048, 'top_logprobs': []}, {'token': ' that', 'bytes': [32, 116, 104, 97, 116], 'logprob': -0.005767734721302986, 'top_logprobs': []}, {'token': ' aids', 'bytes': [32, 97, 105, 100, 115], 'logprob': -2.295668840408325, 'top_logprobs': []}, {'token': ' in', 'bytes': [32, 105, 110], 'logprob': -0.018022222444415092, 'top_logprobs': []}, {'token': ' survival', 'bytes': [32, 115, 117, 114, 118, 105, 118, 97, 108], 'logprob': -0.952755868434906, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.7827375531196594, 'top_logprobs': []}, {'token': ' whether', 'bytes': [32, 119, 104, 101, 116, 104, 101, 114], 'logprob': -3.37805438041687, 'top_logprobs': []}, {'token': ' by', 'bytes': [32, 98, 121], 'logprob': -1.5730986595153809, 'top_logprobs': []}, {'token': ' det', 'bytes': [32, 100, 101, 116], 'logprob': -3.4394097328186035, 'top_logprobs': []}, {'token': 'erring', 'bytes': [101, 114, 114, 105, 110, 103], 'logprob': -3.5358694731257856e-05, 'top_logprobs': []}, {'token': ' predators', 'bytes': [32, 112, 114, 101, 100, 97, 116, 111, 114, 115], 'logprob': -0.005566229112446308, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -0.7694913744926453, 'top_logprobs': []}, {'token': ' attracting', 'bytes': [32, 97, 116, 116, 114, 97, 99, 116, 105, 110, 103], 'logprob': -1.649354100227356, 'top_logprobs': []}, {'token': ' mates', 'bytes': [32, 109, 97, 116, 101, 115], 'logprob': -0.020905843004584312, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.054125040769577026, 'top_logprobs': []}, {'token': '5', 'bytes': [53], 'logprob': -0.15758642554283142, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -6.704273118884885e-07, 'top_logprobs': []}, {'token': ' **', 'bytes': [32, 42, 42], 'logprob': -8.65707261255011e-06, 'top_logprobs': []}, {'token': 'V', 'bytes': [86], 'logprob': -1.7547614574432373, 'top_logprobs': []}, {'token': 'ocal', 'bytes': [111, 99, 97, 108], 'logprob': -7.839121826691553e-05, 'top_logprobs': []}, {'token': ' C', 'bytes': [32, 67], 'logprob': -9999.0, 'top_logprobs': []}, {'token': 'ords', 'bytes': [111, 114, 100, 115], 'logprob': -0.023587265983223915, 'top_logprobs': []}, {'token': '**', 'bytes': [42, 42], 'logprob': -0.5591309070587158, 'top_logprobs': []}, {'token': ':', 'bytes': [58], 'logprob': -5.037835580878891e-05, 'top_logprobs': []}, {'token': ' Unlike', 'bytes': [32, 85, 110, 108, 105, 107, 101], 'logprob': -0.2906014919281006, 'top_logprobs': []}, {'token': ' humans', 'bytes': [32, 104, 117, 109, 97, 110, 115], 'logprob': -0.025780089199543, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.03072812408208847, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.01100537832826376, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': 0.0, 'top_logprobs': []}, {'token': \" don't\", 'bytes': [32, 100, 111, 110, 39, 116], 'logprob': -1.383561372756958, 'top_logprobs': []}, {'token': ' have', 'bytes': [32, 104, 97, 118, 101], 'logprob': -0.021245058625936508, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -0.0001387009397149086, 'top_logprobs': []}, {'token': ' cords', 'bytes': [32, 99, 111, 114, 100, 115], 'logprob': -1.3782830137643032e-05, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.018044814467430115, 'top_logprobs': []}, {'token': ' They', 'bytes': [32, 84, 104, 101, 121], 'logprob': -1.4073907136917114, 'top_logprobs': []}, {'token': ' use', 'bytes': [32, 117, 115, 101], 'logprob': -0.7469758987426758, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.6103420257568359, 'top_logprobs': []}, {'token': ' specialized', 'bytes': [32, 115, 112, 101, 99, 105, 97, 108, 105, 122, 101, 100], 'logprob': -0.45507898926734924, 'top_logprobs': []}, {'token': ' organ', 'bytes': [32, 111, 114, 103, 97, 110], 'logprob': -0.14672505855560303, 'top_logprobs': []}, {'token': ' called', 'bytes': [32, 99, 97, 108, 108, 101, 100], 'logprob': -0.010865417309105396, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -0.008617646060883999, 'top_logprobs': []}, {'token': ' syr', 'bytes': [32, 115, 121, 114], 'logprob': -0.0014270214596763253, 'top_logprobs': []}, {'token': 'inx', 'bytes': [105, 110, 120], 'logprob': 0.0, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.09909570962190628, 'top_logprobs': []}, {'token': ' located', 'bytes': [32, 108, 111, 99, 97, 116, 101, 100], 'logprob': -0.09140513092279434, 'top_logprobs': []}, {'token': ' at', 'bytes': [32, 97, 116], 'logprob': -0.043751310557127, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -4.596781946020201e-05, 'top_logprobs': []}, {'token': ' base', 'bytes': [32, 98, 97, 115, 101], 'logprob': -0.011393333785235882, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -1.9361264946837764e-07, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.20148047804832458, 'top_logprobs': []}, {'token': ' tr', 'bytes': [32, 116, 114], 'logprob': -5.526570384972729e-05, 'top_logprobs': []}, {'token': 'ache', 'bytes': [97, 99, 104, 101], 'logprob': -5.676981345459353e-06, 'top_logprobs': []}, {'token': 'a', 'bytes': [97], 'logprob': -0.0012559457682073116, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.018216552212834358, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.8407076597213745, 'top_logprobs': []}, {'token': ' produce', 'bytes': [32, 112, 114, 111, 100, 117, 99, 101], 'logprob': -0.04040173813700676, 'top_logprobs': []}, {'token': ' sounds', 'bytes': [32, 115, 111, 117, 110, 100, 115], 'logprob': -0.8559735417366028, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.011369053274393082, 'top_logprobs': []}, {'token': ' The', 'bytes': [32, 84, 104, 101], 'logprob': -0.9825559258460999, 'top_logprobs': []}, {'token': ' syr', 'bytes': [32, 115, 121, 114], 'logprob': -0.9239362478256226, 'top_logprobs': []}, {'token': 'inx', 'bytes': [105, 110, 120], 'logprob': -4.36574100604048e-06, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -2.0620975494384766, 'top_logprobs': []}, {'token': ' particularly', 'bytes': [32, 112, 97, 114, 116, 105, 99, 117, 108, 97, 114, 108, 121], 'logprob': -6.921175003051758, 'top_logprobs': []}, {'token': ' versatile', 'bytes': [32, 118, 101, 114, 115, 97, 116, 105, 108, 101], 'logprob': -1.332655429840088, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.3226076066493988, 'top_logprobs': []}, {'token': ' allowing', 'bytes': [32, 97, 108, 108, 111, 119, 105, 110, 103], 'logprob': -0.039798304438591, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.8396608829498291, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': -3.128163257315464e-07, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -0.0012821375858038664, 'top_logprobs': []}, {'token': ' produce', 'bytes': [32, 112, 114, 111, 100, 117, 99, 101], 'logprob': -0.2525489628314972, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.007675130385905504, 'top_logprobs': []}, {'token': ' wide', 'bytes': [32, 119, 105, 100, 101], 'logprob': -0.006461578421294689, 'top_logprobs': []}, {'token': ' range', 'bytes': [32, 114, 97, 110, 103, 101], 'logprob': -0.07799384742975235, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -1.688212614681106e-05, 'top_logprobs': []}, {'token': ' frequencies', 'bytes': [32, 102, 114, 101, 113, 117, 101, 110, 99, 105, 101, 115], 'logprob': -6.449531078338623, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.014200172387063503, 'top_logprobs': []}, {'token': ' tones', 'bytes': [32, 116, 111, 110, 101, 115], 'logprob': -0.7249283194541931, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.6739096641540527, 'top_logprobs': []}, {'token': ' including', 'bytes': [32, 105, 110, 99, 108, 117, 100, 105, 110, 103], 'logprob': -2.692025661468506, 'top_logprobs': []}, {'token': ' those', 'bytes': [32, 116, 104, 111, 115, 101], 'logprob': -0.29618290066719055, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -1.1292153596878052, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.0009753053891472518, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.08582428097724915, 'top_logprobs': []}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -0.0024163227062672377, 'top_logprobs': []}, {'token': 'Overall', 'bytes': [79, 118, 101, 114, 97, 108, 108], 'logprob': -0.4936337172985077, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' the', 'bytes': [32, 116, 104, 101], 'logprob': -1.7197974920272827, 'top_logprobs': []}, {'token': ' ability', 'bytes': [32, 97, 98, 105, 108, 105, 116, 121], 'logprob': -1.2873172760009766, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -0.5392680764198303, 'top_logprobs': []}, {'token': ' parro', 'bytes': [32, 112, 97, 114, 114, 111], 'logprob': -0.0002989968634210527, 'top_logprobs': []}, {'token': 'ts', 'bytes': [116, 115], 'logprob': 0.0, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -9.088346359931165e-07, 'top_logprobs': []}, {'token': ' imitate', 'bytes': [32, 105, 109, 105, 116, 97, 116, 101], 'logprob': -3.766571044921875, 'top_logprobs': []}, {'token': ' human', 'bytes': [32, 104, 117, 109, 97, 110], 'logprob': -0.49825605750083923, 'top_logprobs': []}, {'token': ' speech', 'bytes': [32, 115, 112, 101, 101, 99, 104], 'logprob': -0.08109334856271744, 'top_logprobs': []}, {'token': ' is', 'bytes': [32, 105, 115], 'logprob': -0.052806735038757324, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.05264535918831825, 'top_logprobs': []}, {'token': ' combination', 'bytes': [32, 99, 111, 109, 98, 105, 110, 97, 116, 105, 111, 110], 'logprob': -1.4501025676727295, 'top_logprobs': []}, {'token': ' of', 'bytes': [32, 111, 102], 'logprob': -7.896309739408025e-07, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.021321957930922508, 'top_logprobs': []}, {'token': ' natural', 'bytes': [32, 110, 97, 116, 117, 114, 97, 108], 'logprob': -2.3791329860687256, 'top_logprobs': []}, {'token': ' vocal', 'bytes': [32, 118, 111, 99, 97, 108], 'logprob': -0.27002575993537903, 'top_logprobs': []}, {'token': ' capabilities', 'bytes': [32, 99, 97, 112, 97, 98, 105, 108, 105, 116, 105, 101, 115], 'logprob': -0.961902379989624, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.6326367259025574, 'top_logprobs': []}, {'token': ' their', 'bytes': [32, 116, 104, 101, 105, 114], 'logprob': -0.14378905296325684, 'top_logprobs': []}, {'token': ' social', 'bytes': [32, 115, 111, 99, 105, 97, 108], 'logprob': -0.8751917481422424, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.7200230956077576, 'top_logprobs': []}, {'token': ' intelligent', 'bytes': [32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 116], 'logprob': -0.06344514340162277, 'top_logprobs': []}, {'token': ' nature', 'bytes': [32, 110, 97, 116, 117, 114, 101], 'logprob': -0.038466695696115494, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.09458877891302109, 'top_logprobs': []}], 'refusal': None}\n"
     ]
    }
   ],
   "source": [
    "## openai logprobs\n",
    "\n",
    "model_oai_content_blocks = init_chat_model(\"gpt-4o\", output_version=\"v1\").bind(logprobs=True)\n",
    "\n",
    "response = model_oai_content_blocks.invoke(\"Why do parrots talk?\")\n",
    "print(response.response_metadata[\"logprobs\"])\n",
    "\n",
    "\n",
    "\n",
    "response = model_oai_content_blocks.invoke(\"Why do parrots talk?\")\n",
    "print(response.response_metadata[\"logprobs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545d9b0",
   "metadata": {},
   "source": [
    "## we dont render logprobs\n",
    "\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=05a5b7cc-d554-45de-8505-46174aaabc70&peeked_trace=05a5b7cc-d554-45de-8505-46174aaabc70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b754b",
   "metadata": {},
   "source": [
    "# Multi-modal (constructing messages directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images\n",
    "\n",
    "image_url = \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\"\n",
    "image_base64 = base64.standard_b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
    "        {\"type\": \"image\", \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# From base64 data\n",
    "output = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"base64\": image_base64,\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "        },\n",
    "    ]\n",
    "} \n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def llm_raw(inputs):\n",
    "    return output\n",
    "\n",
    "\n",
    "llm_raw(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e11d0",
   "metadata": {},
   "source": [
    "## Gap in image rendering \n",
    "\n",
    "We don't render images\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=7c87a2bd-fa0e-4aa0-b8b3-2719852a8757&peeked_trace=7c87a2bd-fa0e-4aa0-b8b3-2719852a8757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf\n",
    "\n",
    "pdf_url = \"https://pdfobject.com/pdf/sample.pdf\"\n",
    "pdf_base64 = base64.standard_b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n",
    "        {\"type\": \"file\", \"url\": \"https://pdfobject.com/pdf/sample.pdf\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# From base64 data\n",
    "output = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n",
    "        {\n",
    "            \"type\": \"file\",\n",
    "            \"base64\": pdf_base64,\n",
    "            \"mime_type\": \"application/pdf\",\n",
    "        },\n",
    "    ]\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def llm_raw(inputs):\n",
    "    return output\n",
    "\n",
    "\n",
    "llm_raw(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b755cd",
   "metadata": {},
   "source": [
    "## Gap in redering PDFs\n",
    "\n",
    "Same issue with PDFs\n",
    "\n",
    "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/5b35f59d-3a34-49d1-b6bb-b31d3cf45367?columnVisibilityModel=%7B%2222select%22%3Atrue%2C%22id%22%3Atrue%2C%22status%22%3Atrue%2C%22name%22%3Atrue%2C%22inputs%22%3Atrue%2C%22outputs%22%3Atrue%2C%22start_time%22%3Atrue%2C%22latency%22%3Atrue%2C%22in_dataset%22%3Atrue%2C%22last_queued_at%22%3Atrue%2C%22total_tokens%22%3Atrue%2C%22total_cost%22%3Atrue%2C%22first_token_time%22%3Atrue%2C%22tags%22%3Atrue%2C%22metadata%22%3Atrue%2C%22feedback_stats%22%3Atrue%2C%22reference_example_id%22%3Atrue%2C%22actions%22%3Atrue%7D&timeModel=%7B%22duration%22%3A%227d%22%7D&peek=1955e5c3-b539-4029-8321-25bb25a344b1&peeked_trace=1955e5c3-b539-4029-8321-25bb25a344b1\n",
    "\n",
    "Not going to try audio, video but likely the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import ToolNode, create_agent\n",
    "import random\n",
    "\n",
    "@tool\n",
    "def fetch_user_data(user_id: str) -> str:\n",
    "    \"\"\"Fetch user data from database.\"\"\"\n",
    "    if random.random() > 0.7:\n",
    "        raise ConnectionError(\"Database connection timeout\")\n",
    "    return f\"User {user_id}: John Doe, john@example.com, Active\"\n",
    "\n",
    "@tool\n",
    "def process_transaction(amount: float, user_id: str) -> str:\n",
    "    \"\"\"Process a financial transaction.\"\"\"\n",
    "    if amount > 10000:\n",
    "        raise ValueError(f\"Amount {amount} exceeds maximum limit of 10000\")\n",
    "    return f\"Processed ${amount} for user {user_id}\"\n",
    "\n",
    "def handle_errors(e: Exception) -> str:\n",
    "    if isinstance(e, ConnectionError):\n",
    "        return \"The database is currently overloaded, but it is safe to retry. Please try again with the same parameters.\"\n",
    "    elif isinstance(e, ValueError):\n",
    "        return f\"Error: {e}. Try to process the transaction in smaller amounts.\"\n",
    "    return f\"Error: {e}. Please try again.\"\n",
    "\n",
    "tool_node = ToolNode(\n",
    "    tools=[fetch_user_data, process_transaction],\n",
    "    handle_tool_errors=handle_errors\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    tools=tool_node,\n",
    "    prompt=\"You are a financial assistant.\"\n",
    ")\n",
    "\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Process a payment of 15000 dollars for user123. Generate a receipt email and address it to the user.\"}]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
